{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 14m 38s]\n",
      "val_mae: 0.06659925729036331\n",
      "\n",
      "Best val_mae So Far: 0.06659925729036331\n",
      "Total elapsed time: 00h 15m 15s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "True              |True              |noise_enabled\n",
      "5                 |9                 |num_layers\n",
      "25                |20                |units_0\n",
      "0.00021339        |0.00078283        |lr\n",
      "0.0034495         |0.0024751         |stddev\n",
      "5                 |25                |units_1\n",
      "25                |15                |units_2\n",
      "10                |10                |units_3\n",
      "30                |25                |units_4\n",
      "5                 |20                |units_5\n",
      "20                |20                |units_6\n",
      "10                |25                |units_7\n",
      "15                |20                |units_8\n",
      "1288              |1970              |batch_size\n",
      "143               |247               |n_train\n",
      "6.6558e-05        |4.7138e-06        |alpha\n",
      "\n",
      "Epoch: 1/2000 train loss: 0.11397593 val_mae: 0.29603657\n",
      "Epoch: 2/2000 train loss: 0.06379728 val_mae: 0.26268762\n",
      "Epoch: 3/2000 train loss: 0.03723096 val_mae: 0.23806134\n",
      "Epoch: 4/2000 train loss: 0.02349703 val_mae: 0.21666557\n",
      "Epoch: 5/2000 train loss: 0.02527673 val_mae: 0.20378187\n",
      "Epoch: 6/2000 train loss: 0.01962254 val_mae: 0.19454515\n",
      "Epoch: 7/2000 train loss: 0.01368399 val_mae: 0.18608940\n",
      "Epoch: 8/2000 train loss: 0.01027674 val_mae: 0.17866345\n",
      "Epoch: 9/2000 train loss: 0.00879265 val_mae: 0.17336045\n",
      "Epoch: 10/2000 train loss: 0.00819020 val_mae: 0.16868934\n",
      "Epoch: 11/2000 train loss: 0.00806535 val_mae: 0.16546062\n",
      "Epoch: 12/2000 train loss: 0.00437578 val_mae: 0.16168013\n",
      "Epoch: 13/2000 train loss: 0.00489080 val_mae: 0.15901539\n",
      "Epoch: 14/2000 train loss: 0.00386828 val_mae: 0.15615472\n",
      "Epoch: 15/2000 train loss: 0.00359687 val_mae: 0.15380688\n",
      "Epoch: 16/2000 train loss: 0.00356799 val_mae: 0.15187971\n",
      "Epoch: 17/2000 train loss: 0.00367217 val_mae: 0.15016195\n",
      "Epoch: 18/2000 train loss: 0.00295227 val_mae: 0.14873454\n",
      "Epoch: 19/2000 train loss: 0.00262479 val_mae: 0.14752278\n",
      "Epoch: 20/2000 train loss: 0.00222127 val_mae: 0.14624071\n",
      "Epoch: 21/2000 train loss: 0.00225434 val_mae: 0.14541088\n",
      "Epoch: 22/2000 train loss: 0.00208088 val_mae: 0.14422750\n",
      "Epoch: 23/2000 train loss: 0.00228017 val_mae: 0.14304584\n",
      "Epoch: 24/2000 train loss: 0.00210485 val_mae: 0.14216477\n",
      "Epoch: 25/2000 train loss: 0.00210783 val_mae: 0.14141946\n",
      "Epoch: 26/2000 train loss: 0.00169604 val_mae: 0.14044909\n",
      "Epoch: 27/2000 train loss: 0.00174791 val_mae: 0.13929810\n",
      "Epoch: 28/2000 train loss: 0.00161118 val_mae: 0.13852784\n",
      "Epoch: 29/2000 train loss: 0.00131767 val_mae: 0.13773529\n",
      "Epoch: 30/2000 train loss: 0.00247942 val_mae: 0.13745284\n",
      "Epoch: 31/2000 train loss: 0.00365539 val_mae: 0.13714668\n",
      "Epoch: 32/2000 train loss: 0.00227830 val_mae: 0.13642968\n",
      "Epoch: 33/2000 train loss: 0.00217306 val_mae: 0.13552979\n",
      "Epoch: 34/2000 train loss: 0.00214546 val_mae: 0.13450421\n",
      "Epoch: 35/2000 train loss: 0.00183278 val_mae: 0.13358212\n",
      "Epoch: 36/2000 train loss: 0.00161113 val_mae: 0.13249548\n",
      "Epoch: 37/2000 train loss: 0.00127697 val_mae: 0.13167572\n",
      "Epoch: 38/2000 train loss: 0.00106965 val_mae: 0.13069271\n",
      "Epoch: 39/2000 train loss: 0.00150706 val_mae: 0.12981582\n",
      "Epoch: 40/2000 train loss: 0.00218687 val_mae: 0.12912039\n",
      "Epoch: 41/2000 train loss: 0.00136100 val_mae: 0.12820294\n",
      "Epoch: 42/2000 train loss: 0.00067400 val_mae: 0.12715097\n",
      "Epoch: 43/2000 train loss: 0.00058330 val_mae: 0.12615259\n",
      "Epoch: 44/2000 train loss: 0.00059158 val_mae: 0.12527673\n",
      "Epoch: 45/2000 train loss: 0.00054906 val_mae: 0.12440689\n",
      "Epoch: 46/2000 train loss: 0.00046000 val_mae: 0.12354985\n",
      "Epoch: 47/2000 train loss: 0.00044179 val_mae: 0.12271358\n",
      "Epoch: 48/2000 train loss: 0.00045314 val_mae: 0.12184516\n",
      "Epoch: 49/2000 train loss: 0.00045547 val_mae: 0.12094726\n",
      "Epoch: 50/2000 train loss: 0.00046479 val_mae: 0.11999853\n",
      "Epoch: 51/2000 train loss: 0.00042681 val_mae: 0.11903314\n",
      "Epoch: 52/2000 train loss: 0.00040175 val_mae: 0.11821716\n",
      "Epoch: 53/2000 train loss: 0.00031539 val_mae: 0.11734762\n",
      "Epoch: 54/2000 train loss: 0.00029598 val_mae: 0.11656518\n",
      "Epoch: 55/2000 train loss: 0.00024673 val_mae: 0.11565365\n",
      "Epoch: 56/2000 train loss: 0.00028116 val_mae: 0.11469205\n",
      "Epoch: 57/2000 train loss: 0.00034546 val_mae: 0.11379846\n",
      "Epoch: 58/2000 train loss: 0.00031500 val_mae: 0.11303419\n",
      "Epoch: 59/2000 train loss: 0.00030085 val_mae: 0.11228231\n",
      "Epoch: 60/2000 train loss: 0.00025494 val_mae: 0.11140190\n",
      "Epoch: 61/2000 train loss: 0.00021535 val_mae: 0.11066815\n",
      "Epoch: 62/2000 train loss: 0.00019960 val_mae: 0.10986922\n",
      "Epoch: 63/2000 train loss: 0.00018795 val_mae: 0.10901589\n",
      "Epoch: 64/2000 train loss: 0.00019873 val_mae: 0.10825451\n",
      "Epoch: 65/2000 train loss: 0.00019001 val_mae: 0.10750119\n",
      "Epoch: 66/2000 train loss: 0.00016974 val_mae: 0.10674923\n",
      "Epoch: 67/2000 train loss: 0.00015437 val_mae: 0.10604613\n",
      "Epoch: 68/2000 train loss: 0.00016370 val_mae: 0.10537409\n",
      "Epoch: 69/2000 train loss: 0.00010943 val_mae: 0.10468131\n",
      "Epoch: 70/2000 train loss: 0.00014968 val_mae: 0.10413634\n",
      "Epoch: 71/2000 train loss: 0.00014260 val_mae: 0.10357857\n",
      "Epoch: 72/2000 train loss: 0.00013388 val_mae: 0.10307831\n",
      "Epoch: 73/2000 train loss: 0.00012909 val_mae: 0.10257438\n",
      "Epoch: 74/2000 train loss: 0.00011259 val_mae: 0.10221365\n",
      "Epoch: 75/2000 train loss: 0.00010547 val_mae: 0.10185924\n",
      "Epoch: 76/2000 train loss: 0.00010607 val_mae: 0.10161373\n",
      "Epoch: 77/2000 train loss: 0.00011365 val_mae: 0.10144410\n",
      "Epoch: 78/2000 train loss: 0.00008249 val_mae: 0.10127046\n",
      "Epoch: 79/2000 train loss: 0.00005582 val_mae: 0.10106359\n",
      "Epoch: 80/2000 train loss: 0.00004798 val_mae: 0.10095795\n",
      "Epoch: 81/2000 train loss: 0.00003831 val_mae: 0.10077045\n",
      "Epoch: 82/2000 train loss: 0.00003666 val_mae: 0.10063926\n",
      "Epoch: 83/2000 train loss: 0.00003268 val_mae: 0.10057103\n",
      "Epoch: 84/2000 train loss: 0.00003384 val_mae: 0.10038432\n",
      "Epoch: 85/2000 train loss: 0.00003178 val_mae: 0.10024823\n",
      "Epoch: 86/2000 train loss: 0.00003180 val_mae: 0.10002603\n",
      "Epoch: 87/2000 train loss: 0.00004657 val_mae: 0.09995211\n",
      "Epoch: 88/2000 train loss: 0.00020736 val_mae: 0.09972914\n",
      "Epoch: 89/2000 train loss: 0.00027725 val_mae: 0.09941229\n",
      "Epoch: 90/2000 train loss: 0.00016187 val_mae: 0.09914056\n",
      "Epoch: 91/2000 train loss: 0.00010981 val_mae: 0.09887406\n",
      "Epoch: 92/2000 train loss: 0.00008658 val_mae: 0.09855542\n",
      "Epoch: 93/2000 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\differentiate\\hypertuning_3.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 305>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=293'>294</a>\u001b[0m tuner \u001b[39m=\u001b[39m kt\u001b[39m.\u001b[39mRandomSearch(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=294'>295</a>\u001b[0m     MyHyperModel(),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=295'>296</a>\u001b[0m     objective\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_mae\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=300'>301</a>\u001b[0m     project_name\u001b[39m=\u001b[39mproject_name,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=301'>302</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=303'>304</a>\u001b[0m \u001b[39m# tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=304'>305</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(tf_coords\u001b[39m=\u001b[39;49mtf_coords,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=305'>306</a>\u001b[0m              tf_boundary_coords\u001b[39m=\u001b[39;49mtf_boundary_coords)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:179\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/base_tuner.py?line=175'>176</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/base_tuner.py?line=177'>178</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/base_tuner.py?line=178'>179</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/base_tuner.py?line=179'>180</a>\u001b[0m \u001b[39m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/base_tuner.py?line=180'>181</a>\u001b[0m \u001b[39mif\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:294\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=291'>292</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=292'>293</a>\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[1;32m--> <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=293'>294</a>\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_and_fit_model(trial, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=295'>296</a>\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=296'>297</a>\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:222\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=219'>220</a>\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=220'>221</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=221'>222</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mfit(hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=222'>223</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tuner_utils\u001b[39m.\u001b[39mconvert_to_metrics_dict(\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=223'>224</a>\u001b[0m     results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mHyperModel.fit()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/keras_tuner/engine/tuner.py?line=224'>225</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\differentiate\\hypertuning_3.ipynb Cell 1'\u001b[0m in \u001b[0;36mMyHyperModel.fit\u001b[1;34m(self, hp, model, tf_coords, tf_boundary_coords, validation_coords, patience, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=234'>235</a>\u001b[0m tf_sample_coords \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=235'>236</a>\u001b[0m     [tf_coords[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=236'>237</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(hp\u001b[39m.\u001b[39mInt(\u001b[39m'\u001b[39m\u001b[39mn_train\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m500\u001b[39m)):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=237'>238</a>\u001b[0m     metrics, train_loss \u001b[39m=\u001b[39m train_step(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=238'>239</a>\u001b[0m         hp, model, tf_sample_coords, tf_boundary_coords, batch_size)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=239'>240</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=240'>241</a>\u001b[0m mean_train_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(train_losses)\n",
      "\u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\differentiate\\hypertuning_3.ipynb Cell 1'\u001b[0m in \u001b[0;36mMyHyperModel.fit.<locals>.train_step\u001b[1;34m(hp, model, tf_sample_coords, tf_boundary_coords, batch_size)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=197'>198</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=198'>199</a>\u001b[0m     \u001b[39m# this custom_loss function shall be added to keras.losses and use the loss defined in compile\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=199'>200</a>\u001b[0m     loss, res \u001b[39m=\u001b[39m custom_loss()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=201'>202</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, model\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=202'>203</a>\u001b[0m model\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mapply_gradients(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=203'>204</a>\u001b[0m     \u001b[39mzip\u001b[39m(gradients, model\u001b[39m.\u001b[39mtrainable_variables))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/hypertuning_3.ipynb#ch0000000?line=204'>205</a>\u001b[0m \u001b[39m# Update metrics (includes the metric that tracks the loss)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1100\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1093'>1094</a>\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1094'>1095</a>\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1095'>1096</a>\u001b[0m           output_gradients))\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1096'>1097</a>\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1097'>1098</a>\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1099'>1100</a>\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1100'>1101</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1101'>1102</a>\u001b[0m     flat_targets,\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1102'>1103</a>\u001b[0m     flat_sources,\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1103'>1104</a>\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1104'>1105</a>\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1105'>1106</a>\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1108'>1109</a>\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=1109'>1110</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=62'>63</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=63'>64</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=64'>65</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=67'>68</a>\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=68'>69</a>\u001b[0m     target,\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=69'>70</a>\u001b[0m     sources,\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=70'>71</a>\u001b[0m     output_gradients,\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=71'>72</a>\u001b[0m     sources_raw,\n\u001b[0;32m     <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=72'>73</a>\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:647\u001b[0m, in \u001b[0;36m_aggregate_grads\u001b[1;34m(gradients)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=644'>645</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gradients[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=645'>646</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(g, ops\u001b[39m.\u001b[39mTensor) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m gradients):\n\u001b[1;32m--> <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=646'>647</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49madd_n(gradients)\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=647'>648</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=648'>649</a>\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=649'>650</a>\u001b[0m       \u001b[39misinstance\u001b[39m(g, (ops\u001b[39m.\u001b[39mTensor, indexed_slices\u001b[39m.\u001b[39mIndexedSlices))\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/eager/backprop.py?line=650'>651</a>\u001b[0m       \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m gradients)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:393\u001b[0m, in \u001b[0;36madd_n\u001b[1;34m(inputs, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=390'>391</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=391'>392</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=392'>393</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=393'>394</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mAddN\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, inputs)\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=394'>395</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    <a href='file:///c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/venv_tsunami/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=395'>396</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sympy import Matrix\n",
    "import os\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import keras\n",
    "import sympy as sm\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "print(device_lib.list_local_devices())\n",
    "# what if empty...\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# On windows systems you cannont install NCCL that is required for multi GPU\n",
    "# So we need to follow hierarchical copy method or reduce to single GPU (less efficient than the former)\n",
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    devices=['GPU:0'], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "DTYPE = 'float32'\n",
    "\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "\n",
    "__file__ = 'C:/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami'\n",
    "\n",
    "print('\\n cwd:', os.getcwd())\n",
    "os.chdir(__file__)\n",
    "print('changed to:', os.getcwd(), '\\n')\n",
    "\n",
    "# where we'll put the results and save the models\n",
    "directory = \"differentiate/my_dir\"\n",
    "project_name = \"tune_hypermodel\"\n",
    "path_to_trials = __file__+'/'+directory+'/' + project_name\n",
    "\n",
    "first_run = True  # wait that it creates the folder trial_O... first and nothing else first\n",
    "\n",
    "overwrite = True  # true = destroy previous results, false = resume search\n",
    "\n",
    "#############################################################################\n",
    "# Set F here\n",
    "\n",
    "x, y = sm.symbols('x,y')\n",
    "\n",
    "\n",
    "def expr_dummy_F():\n",
    "    return x*(1-x)*y*(1-y)\n",
    "\n",
    "\n",
    "expr_F = expr_dummy_F()\n",
    "dexpr_F_dx = sm.diff(expr_F, x, 1)\n",
    "dexpr_F_dxx = sm.diff(dexpr_F_dx, x, 1)\n",
    "dexpr_F_dy = sm.diff(expr_F, y, 1)\n",
    "dexpr_F_dyy = sm.diff(dexpr_F_dy, y, 1)\n",
    "\n",
    "# print(dexpr_F_dx)\n",
    "# print(dexpr_F_dxx)\n",
    "\n",
    "# You can forget a no lambdified expression => here we greatly avoid 'for' loops\n",
    "\n",
    "expr_F = sm.lambdify([x, y], Matrix([expr_F]), 'numpy')\n",
    "dexpr_F_dx = sm.lambdify([x, y], Matrix([dexpr_F_dx]), 'numpy')\n",
    "dexpr_F_dxx = sm.lambdify([x, y], Matrix([dexpr_F_dxx]), 'numpy')\n",
    "dexpr_F_dy = sm.lambdify([x, y], Matrix([dexpr_F_dy]), 'numpy')\n",
    "dexpr_F_dyy = sm.lambdify([x, y], Matrix([dexpr_F_dyy]), 'numpy')\n",
    "\n",
    "\n",
    "def evaluate_F_and_diff(X):\n",
    "    F = tf.squeeze(tf.transpose(expr_F(X[:, 0], X[:, 1])), axis=-1)\n",
    "    dF_dx = tf.expand_dims(dexpr_F_dx(X[:, 0], X[:, 1]), axis=-1)\n",
    "    dF_dxx = tf.expand_dims(dexpr_F_dxx(X[:, 0], X[:, 1]), axis=-1)\n",
    "    dF_dy = tf.expand_dims(dexpr_F_dy(X[:, 0], X[:, 1]), axis=-1)\n",
    "    dF_dyy = tf.expand_dims(dexpr_F_dyy(X[:, 0], X[:, 1]), axis=-1)\n",
    "\n",
    "    return F, dF_dx, dF_dxx, dF_dy, dF_dyy\n",
    "\n",
    " # oddly enough expr_F and dexpr_F_d... do not have the same output\n",
    "\n",
    "#############################################################################\n",
    "# Set A here\n",
    "\n",
    "\n",
    "A = 0\n",
    "dA_dxx = 0\n",
    "dA_dyy = 0\n",
    "#############################################################################\n",
    "# Given EDO\n",
    "\n",
    "\n",
    "def f(X):\n",
    "    return tf.sin(np.pi*X[:, 0])*tf.sin(np.pi*X[:, 1])\n",
    "\n",
    "\n",
    "def boundary_conditions(X):\n",
    "    return 0\n",
    "\n",
    "\n",
    "def residual(du_dxx, du_dyy, f_ind):\n",
    "    return du_dxx+du_dyy+f_ind\n",
    "\n",
    "\n",
    "def differentiate(model, x):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        x1, x2 = x[:, 0:1], x[:, 1:2]\n",
    "        tape.watch(x1)\n",
    "        tape.watch(x2)\n",
    "        u = model(tf.stack([x1[:, 0], x2[:, 0]], axis=1))\n",
    "        du_dx = tape.gradient(u, x1)\n",
    "        du_dy = tape.gradient(u, x2)\n",
    "    du_dxx = tape.gradient(du_dx, x1)\n",
    "    du_dyy = tape.gradient(du_dy, x2)\n",
    "    return du_dx, du_dxx, du_dy, du_dyy\n",
    "\n",
    "\n",
    "grid_length = 100\n",
    "\n",
    "\n",
    "X = np.linspace(0, 1, grid_length, endpoint=True)\n",
    "Y = np.linspace(0, 1, grid_length, endpoint=True)\n",
    "tf_coords = tf.convert_to_tensor(\n",
    "    [tf.constant([x, y], dtype=DTYPE) for x in X for y in Y])\n",
    "tf_boundary_coords = tf.convert_to_tensor([tf.constant([x, y], dtype=DTYPE) for x in [\n",
    "                                          0, 1] for y in Y] + [tf.constant([x, y], dtype=DTYPE) for y in [0, 1] for x in X])\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp, dim=2):\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Input(shape=(dim))\n",
    "        ])\n",
    "\n",
    "        if hp.Boolean('noise_enabled'):\n",
    "            model.add(keras.layers.GaussianNoise(stddev=hp.Float(\n",
    "                \"stddev\", min_value=1e-4, max_value=1e-2, sampling=\"log\")))\n",
    "\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 10)):\n",
    "            model.add(\n",
    "                keras.layers.Dense(units=hp.Int(f\"units_{i}\", min_value=5, max_value=30, step=5),\n",
    "                                   activation='elu', kernel_initializer='he_normal'\n",
    "                                   )\n",
    "            )\n",
    "        model.add(keras.layers.Dense(1, use_bias=False))\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(\n",
    "            learning_rate=hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, tf_coords, tf_boundary_coords, validation_coords=[], patience=10, *args, **kwargs):\n",
    "        def save_model(model):\n",
    "            # checkpoint to save trained model\n",
    "            last_trial_folder = [f for f in os.listdir(\n",
    "                path_to_trials) if f[:5] == 'trial'][-1]\n",
    "            path_to_last_trial = path_to_trials+'/'+last_trial_folder\n",
    "            if not('checkpoint.h5' in os.listdir(path_to_last_trial)):\n",
    "                model.save(path_to_trials+'/' +\n",
    "                           last_trial_folder+'/checkpoint.h5')\n",
    "\n",
    "        # @tf.function\n",
    "        def train_step(hp, model, tf_sample_coords, tf_boundary_coords, batch_size):\n",
    "            def g_3(X):\n",
    "                # F_x = Pstud._eval_polynome_numpy(F_xpy_real,x[0,0],x[0,1])\n",
    "                N_X = model(X)\n",
    "                return tf.squeeze(tf.transpose(expr_F(X[:, 0], X[:, 1])), axis=-1)*N_X\n",
    "\n",
    "            def custom_loss():\n",
    "                _, dg_dxx, _, dg_dyy = differentiate(g_3, tf_sample_coords)\n",
    "                f_r = tf.reshape(f(tf_sample_coords), [batch_size, 1])\n",
    "                res = residual(dg_dxx, dg_dyy, f_r)\n",
    "\n",
    "                alpha = hp.Float('alpha', min_value=1e-6,\n",
    "                                 max_value=3, sampling='log')\n",
    "                loss = tf.reduce_mean(tf.square(res)) + alpha*tf.reduce_mean(\n",
    "                    tf.square(g_3(tf_boundary_coords)-boundary_conditions(tf_boundary_coords)))\n",
    "                return loss, res\n",
    "\n",
    "            def custom_loss_3():\n",
    "                dN_dx, dN_dxx, dN_dy, dN_dyy = differentiate(\n",
    "                    model, tf_sample_coords)\n",
    "                f_r = tf.reshape(f(tf_sample_coords), [batch_size, 1])\n",
    "\n",
    "                F, dF_dx, dF_dxx, dF_dy, dF_dyy = evaluate_F_and_diff(\n",
    "                    tf_sample_coords)\n",
    "\n",
    "                dg_dxx = dF_dxx + 2*dF_dx*dN_dx + F*dN_dxx + dA_dxx\n",
    "                dg_dyy = dF_dyy + 2*dF_dy*dN_dy + F*dN_dyy + dA_dyy\n",
    "                res = residual(dg_dxx, dg_dyy, f_r)\n",
    "\n",
    "                loss = tf.reduce_mean(tf.square(res))\n",
    "                return loss, res\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # this custom_loss function shall be added to keras.losses and use the loss defined in compile\n",
    "                loss, res = custom_loss()\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(\n",
    "                zip(gradients, model.trainable_variables))\n",
    "            # Update metrics (includes the metric that tracks the loss)\n",
    "            model.compiled_metrics.update_state(res, tf.zeros(tf.shape(res)))\n",
    "            # Return a dict mapping metric names to current value\n",
    "            return {m.name: m.result() for m in model.metrics}, loss\n",
    "\n",
    "        # @tf.function\n",
    "        def validate(model, validation_coords):\n",
    "            def g_3(x):\n",
    "                N_x = model(x, training=False)\n",
    "                return N_x\n",
    "\n",
    "            _, dg_dxx, _, dg_dyy = differentiate(g_3, validation_coords)\n",
    "            f_r = tf.reshape(f(validation_coords), [\n",
    "                tf.shape(validation_coords)[0], 1])\n",
    "            res = residual(dg_dxx, dg_dyy, f_r)\n",
    "            model.compiled_metrics.update_state(res, tf.zeros(tf.shape(res)))\n",
    "            return {m.name: m.result() for m in model.metrics}\n",
    "\n",
    "        history = {'train_loss': [],\n",
    "                   'val_mae': []}\n",
    "        n_epoch = 2000\n",
    "        for epoch in range(1, n_epoch+1):\n",
    "            EarlyStopped = False\n",
    "\n",
    "            print(f'Epoch: {epoch}/{n_epoch}', end=' ')\n",
    "            train_losses, val_losses, val_maes = [], [], []\n",
    "            batch_size = hp.Int(\n",
    "                'batch_size', 10, (tf.shape(tf_coords).numpy()[0]-1)//5)\n",
    "            indices = np.random.randint(\n",
    "                tf_coords.shape[0], size=batch_size)\n",
    "            tf_sample_coords = tf.convert_to_tensor(\n",
    "                [tf_coords[i] for i in indices])\n",
    "            for _ in range(hp.Int('n_train', 10, 500)):\n",
    "                metrics, train_loss = train_step(\n",
    "                    hp, model, tf_sample_coords, tf_boundary_coords, batch_size)\n",
    "                train_losses.append(train_loss)\n",
    "            mean_train_loss = np.mean(train_losses)\n",
    "            history['train_loss'].append(mean_train_loss)\n",
    "            # .3f pour 3 chiffres après la virgule\n",
    "            print(f'train loss: {mean_train_loss:.8f}', end=' ')\n",
    "\n",
    "            if tf.shape(validation_coords).numpy()[0]:\n",
    "                val_metrics = validate(model, validation_coords)\n",
    "                print(val_metrics)\n",
    "                val_mae = val_metrics['mae']\n",
    "                history['val_mae'].append(\n",
    "                    val_mae)\n",
    "                print(\n",
    "                    f'val_mae: {val_mae:.3f}')\n",
    "            else:\n",
    "                # create validation set of size batch_size from tf_coords modified\n",
    "                # random => no chance to be the same as the training set\n",
    "                # remark: points of validation set could exceed the domain of definition\n",
    "                indices = np.random.randint(\n",
    "                    tf_coords.shape[0], size=batch_size)\n",
    "                tf_val_coords = tf.convert_to_tensor(\n",
    "                    [tf_coords[i] for i in indices])\n",
    "                tf_val_coords = tf_val_coords + tf.random.normal(shape=tf.shape(\n",
    "                    tf_val_coords).numpy(), mean=0, stddev=1)\n",
    "                val_metrics = validate(model, tf_val_coords)\n",
    "                val_mae = val_metrics['mae']\n",
    "                history['val_mae'].append(\n",
    "                    val_mae)\n",
    "                print(\n",
    "                    f'val_mae: {val_mae:.8f}')\n",
    "\n",
    "            # EarlyStopping implemented :\n",
    "            if (len(history['val_mae']) > (patience+1)) and np.argmin(history['val_mae'][-(patience+1):]) == 0:\n",
    "                EarlyStopped = True\n",
    "                if not(first_run):\n",
    "                    save_model(model)\n",
    "                break\n",
    "\n",
    "        if not(EarlyStopped) and not(first_run):\n",
    "            save_model(model)\n",
    "        return history\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    hp = kt.HyperParameters()\n",
    "    hypermodel = MyHyperModel()\n",
    "    model = hypermodel.build(hp)\n",
    "history = hypermodel.fit(hp, model, tf_coords=tf_coords,\n",
    "                         tf_boundary_coords=tf_boundary_coords)\n",
    "\n",
    "\n",
    "# keep exuctions_per_trial to 1 ! to overwrite with same sample size\n",
    "# patience à 30\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=500,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=overwrite,\n",
    "    directory=directory,\n",
    "    project_name=project_name,\n",
    ")\n",
    "\n",
    "# tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))\n",
    "tuner.search(tf_coords=tf_coords,\n",
    "             tf_boundary_coords=tf_boundary_coords)\n",
    "\n",
    "# print('search_space_summary:\\n', tuner.search_space_summary(), end='\\n')\n",
    "\n",
    "\n",
    "# hypermodel = MyHyperModel()\n",
    "# best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# # model = hypermodel.build(best_hp)\n",
    "# model = keras.models.load_model(\n",
    "#     'differentiate/my_dir/tune_hypermodel/trial_0/checkpoint.h5')\n",
    "# hypermodel.fit(best_hp, model, tf_coords=tf_coords,\n",
    "#                tf_boundary_coords=tf_boundary_coords)\n",
    "\n",
    "# print(best_hp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 models.\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_model = models[0]\n",
    "# Build the model.\n",
    "# Needed for `Sequential` without specified `input_shape`.\n",
    "best_model.build(dim=2)\n",
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed45ddf3773bc83c4383a1a855f930bc53f0b7f7e989df0f3182f9bb62c1b41f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv_tsunami': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
