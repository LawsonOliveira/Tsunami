{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "import jax, optax, flax\n",
    "import pickle\n",
    "import functools\n",
    "import matplotlib.pyplot, matplotlib.animation\n",
    "import numpy\n",
    "\n",
    "# Set and verify device\n",
    "jax.config.update('jax_platform_name', 'gpu')\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update('jax_disable_jit', True) # Desactive the compilation for better debugging\n",
    "print(jax.lib.xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(flax.linen.Module):\n",
    "    \"\"\"\n",
    "        Create a multilayer perceptron and initialize the neural network\n",
    "    Inputs :\n",
    "        A SEED number and the layers structure, activation function and a bool to indicates if it's occurring a training\n",
    "     \"\"\"\n",
    "    layers: list\n",
    "    training: bool\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Compute the output of the neural network\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[[batch_size,batch_size,batch_size]]\n",
    "            -- coordinates and time  (x,y,t)\n",
    "        Returns\n",
    "        -------\n",
    "        x : jax.numpy.ndarray[[batch_size,batch_size,batch_size]]\n",
    "            -- numerical output of the neural network\t(u,v,p)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = flax.linen.BatchNorm(use_running_average=not self.training)(x)\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            x = flax.linen.Dense(self.layers[i])(x)\n",
    "            x = flax.linen.BatchNorm(use_running_average=not self.training)(x)\n",
    "            x = flax.linen.tanh(x)\n",
    "            #x = nn.Dropout(rate=0.5, deterministic=not self.training)(x)\n",
    "        x = flax.linen.Dense(self.layers[-1])(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDE Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDE_operators:\n",
    "    \"\"\"\n",
    "        Class with the most common operators used to solve PDEs\n",
    "    Input:\n",
    "        A function that we want to compute the respective operator\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class initialization\n",
    "    def __init__(self, psi_function, c_function, cg_function, solution):\n",
    "        self.psi_function = psi_function\n",
    "        self.c_function = c_function\n",
    "        self.cg_function = cg_function\n",
    "        self.solution = solution\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def comp_grad_psi(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the psi function\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inputs : jax.numpy.ndarray[[batch_size,batch_size]]\n",
    "            -- coordinates and time  (x,y)\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : jax.numpy.ndarray[[batch_size,batch_size]]\n",
    "            -- numerical values of the gradient applied to the inputs\n",
    "        \"\"\"\n",
    "\n",
    "        fun = lambda params, x, y: self.psi_function(params, x, y)\n",
    "\n",
    "        @functools.partial(jax.jit)    \n",
    "        def action(params, x, y):               # function to vectorize the laplacian\n",
    "            u_x = jax.jacfwd(fun, 1)(params, x, y)\n",
    "            u_y = jax.jacfwd(fun, 2)(params, x, y)\n",
    "            return jax.numpy.column_stack((u_x, u_y))\n",
    "\n",
    "        vec_fun = jax.vmap(action, in_axes = (None, 0, 0))\n",
    "        gradient = vec_fun(params, inputs[:,0], inputs[:,1])\n",
    "        gradient = gradient.reshape((gradient.shape[0], gradient.shape[2]))\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def comp_grad_c_cg_psi(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Compute the gradient nabla(c*cg*psi)\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inputs : jax.numpy.ndarray[[batch_size,batch_size,batch_size]]\n",
    "            -- coordinates and time  (x,y)\n",
    "        Returns\n",
    "        -------\n",
    "        res : jax.numpy.ndarray[[batch_size,batch_size]]\n",
    "            -- numerical values of the gradient nabla(c*cg*psi) applied to the inputs\n",
    "        \"\"\"\n",
    "\n",
    "        fun = lambda params, x, y: self.psi_function(params, x, y)*self.c_function(x,y)*self.cg_function(x,y)\n",
    "\n",
    "        @functools.partial(jax.jit)    \n",
    "        def action(params, x, y):               # function to vectorize the laplacian\n",
    "            u_x = jax.jacfwd(fun, 1)(params, x, y)\n",
    "            u_y = jax.jacfwd(fun, 2)(params, x, y)\n",
    "            return jax.numpy.column_stack((u_x, u_y))\n",
    "\n",
    "        vec_fun = jax.vmap(action, in_axes = (None, 0, 0))\n",
    "        gradient = vec_fun(params, inputs[:,0], inputs[:,1])\n",
    "        gradient = gradient.reshape((gradient.shape[0], gradient.shape[2]))\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def comp_grad_c_cg(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Compute the gradient nabla(c*cg)\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inputs : jax.numpy.ndarray[[batch_size,batch_size,batch_size]]\n",
    "            -- coordinates and time  (x,y)\n",
    "        Returns\n",
    "        -------\n",
    "        res : jax.numpy.ndarray[[batch_size,batch_size]]\n",
    "            -- numerical values of the gradient nabla(c*cg) applied to the inputs\n",
    "        \"\"\"\n",
    "\n",
    "        fun = lambda params, x, y: self.c_function(x,y)*self.cg_function(x,y)\n",
    "\n",
    "        @functools.partial(jax.jit)    \n",
    "        def action(params, x, y):               # function to vectorize the laplacian\n",
    "            u_x = jax.jacfwd(fun, 1)(params, x, y)\n",
    "            u_y = jax.jacfwd(fun, 2)(params, x, y)\n",
    "            return jax.numpy.column_stack((u_x, u_y))\n",
    "\n",
    "        vec_fun = jax.vmap(action, in_axes = (None, 0, 0))\n",
    "        gradient = vec_fun(params, inputs[:,0], inputs[:,1])\n",
    "        gradient = gradient.reshape((gradient.shape[0], gradient.shape[2]))\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def dsol_dt(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Compute the time derivative os the solution\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inputs : jax.numpy.ndarray[[batch_size,batch_size,batch_size,batch_size]]\n",
    "            -- coordinates and time  (x,y,z,t)\n",
    "        Returns\n",
    "        -------\n",
    "        res : jax.numpy.ndarray[batch_size]\n",
    "            -- numerical values of the time derivative applied to the inputs\n",
    "        \"\"\"\n",
    "\n",
    "        fun = lambda params, x, y, z, t: self.solution(params, x, y, z, t)\n",
    "\n",
    "        @functools.partial(jax.jit)    \n",
    "        def action(params, x, y, z, t):               # function to vectorize the laplacian\n",
    "            u_t = jax.jacfwd(fun, 4)(params, x, y, z, t)\n",
    "            return u_t\n",
    "\n",
    "        vec_fun = jax.vmap(action, in_axes = (None, 0, 0, 0, 0))\n",
    "        res = vec_fun(params, inputs[:,0], inputs[:,1], inputs[:,2], inputs[:,3])\n",
    "\n",
    "        return res.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    \"\"\"\n",
    "    Solve a PDE using Physics Informed Neural Networks   \n",
    "    Input:\n",
    "        The evaluation function of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # Class initialization\n",
    "    def __init__(self, NN_evaluation, optimizer):\n",
    "        self.NN_evaluation = NN_evaluation\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.operators = PDE_operators(self.spatial_solution2d, self.c_coeff, self.cg_coeff, self.solution)\n",
    "        self.gradient_psi = self.operators.comp_grad_psi\n",
    "        self.dsol_dt = self.operators.dsol_dt\n",
    "        self.gradient_c_cg_psi = self.operators.comp_grad_c_cg_psi\n",
    "        self.gradient_c_cg = self.operators.comp_grad_c_cg\n",
    "    \n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    ################### Solution domain 2d ##################\n",
    "    #########################################################\n",
    "    def spatial_solution2d(self, params, inputX, inputY):\n",
    "        \"\"\"\n",
    "        Compute the solution of the PDE on the points (x,y)\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inputX : jax.numpy.array[batch_size]\n",
    "            -- points on the x-axis of the mesh\n",
    "        inputY : jax.numpy.array[batch_size]\n",
    "            -- points on the y-axis of the mesh\n",
    "        Returns\n",
    "        -------\n",
    "        applied_solution : jax.numpy.ndarray[batch_size]\n",
    "            -- PINN solution applied to inputs. return u, a complex number\n",
    "        \"\"\"\n",
    "        inputs = jax.numpy.column_stack((inputX, inputY))\n",
    "        NN = jax.vmap(functools.partial(jax.jit(self.NN_evaluation), params))(inputs)\n",
    "        NN_output = jax.lax.complex(NN[:,0],NN[:,1])   # The first output of the NN is the real part, the second is the imaginary part\n",
    "        \n",
    "        return NN_output\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def loss_residual_pde(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Compute the residual of the pde inside the domain\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inside : jax.numpy.ndarray[[batch_size, batch_size]]\n",
    "            -- (x,y) points from the mesh\n",
    "        Returns\n",
    "        -------\n",
    "        loss_residual : a float.64\n",
    "            -- loss function applied to inputs\n",
    "        \"\"\"\n",
    "\n",
    "        pred_1 = self.k_coeff(inputs[:,0],inputs[:,1])**2*self.c_coeff(inputs[:,0],inputs[:,1])*self.cg_coeff(inputs[:,0],inputs[:,1])*self.spatial_solution2d(params, inputs[:,0], inputs[:,1])**2\n",
    "        pred_2 = 0 #temporary, H constant implies grad(c*c_g) = 0, grad function not working\n",
    "        #pred_2 = self.spatial_solution2d(params, inputs[:,0], inputs[:,1])*jax.numpy.einsum('ij,ij->i', self.gradient_psi(params, inputs), self.gradient_c_cg(params, inputs))\n",
    "        pred_3 = jax.numpy.einsum('ij,ij->i', self.gradient_psi(params, inputs), self.gradient_c_cg_psi(params,inputs))\n",
    "        loss_value = jax.numpy.mean(pred_1 + pred_2 - pred_3)\n",
    "        \n",
    "        return loss_value\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def loss_absorbing_boundary(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Compute the loss function at the top, bottom and right boundaries\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        bound : jax.numpy.ndarray[[batch_size, batch_size]]\n",
    "            -- (x,y,t) points from boundary\n",
    "        Returns\n",
    "        -------\n",
    "        loss_absorbing : a float.64\n",
    "            -- loss function applied to inputs\n",
    "        \"\"\"\n",
    "        preds_absorbing = jax.lax.complex(-jax.numpy.imag(self.k_coeff(inputs[:,0], inputs[:,1])*self.c_coeff(inputs[:,0], inputs[:,1])*self.cg_coeff(inputs[:,0], inputs[:,1])*self.spatial_solution2d(params, inputs[:,0], inputs[:,1])**2), jax.numpy.real(self.k_coeff(inputs[:,0], inputs[:,1])*self.c_coeff(inputs[:,0], inputs[:,1])*self.cg_coeff(inputs[:,0], inputs[:,1])*self.spatial_solution2d(params, inputs[:,0], inputs[:,1])**2))\n",
    "        loss_absorbing = jax.numpy.mean(preds_absorbing)\n",
    "      \n",
    "        return loss_absorbing\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def loss_incident_boundary(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Compute the loss function at the left open boundary\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        bound : jax.numpy.ndarray[[batch_size, batch_size]]\n",
    "            -- (x,y,t) points from boundary\n",
    "        Returns\n",
    "        -------\n",
    "        loss_incident : a float.64\n",
    "            -- loss function applied to inputs\n",
    "        \"\"\"\n",
    "        preds_incident = jax.lax.complex(-jax.numpy.imag(self.k_coeff(inputs[:,0], inputs[:,1])*self.c_coeff(inputs[:,0], inputs[:,1])*self.cg_coeff(inputs[:,0], inputs[:,1])*self.spatial_solution2d(params, inputs[:,0], inputs[:,1])*(2*self.incident_psi(params, inputs)-self.spatial_solution2d(params, inputs[:,0], inputs[:,1]))), jax.numpy.real(self.k_coeff(inputs[:,0], inputs[:,1])*(2*self.incident_psi(params, inputs)-self.spatial_solution2d(params, inputs[:,0], inputs[:,1]))))\n",
    "        loss_incident = jax.numpy.mean(preds_incident)\n",
    "      \n",
    "        return loss_incident\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def loss_function(self, params, inside, reflective, absorbing, incident):\n",
    "        \"\"\"\n",
    "        Compute the sum of each loss function\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inside : jax.numpy.ndarray[[batch_size, batch_size,batch_size]]\n",
    "            -- (x,y,t) points from the mesh\n",
    "        bound : jax.numpy.ndarray[[batch_size, batch_size,batch_size]]\n",
    "            -- (x,y,t) points from boundary\n",
    "        initial : jax.numpy.ndarray[[batch_size, batch_size, batch_size]]\n",
    "            -- (x,y,t) points from initial condition\n",
    "        Returns\n",
    "        -------\n",
    "        loss_sum : a float.64\n",
    "            -- loss function applied to inputs\n",
    "        losses : numpy.array(loss_residual, loss_bound, loss_front_behind)\n",
    "            -- current values of each loss function\n",
    "        \"\"\"\n",
    "\n",
    "        loss1 = self.loss_incident_boundary(params, incident) \n",
    "        loss2 = self.loss_absorbing_boundary(params, absorbing)\n",
    "\n",
    "        loss_bound = loss1 + loss2\n",
    "        loss_residual = self.loss_residual_pde(params, inside)\n",
    "        loss_sum = loss_bound + loss_residual\n",
    "        losses = jax.numpy.array([loss_residual, loss_bound])\n",
    "\n",
    "        return loss_sum, losses\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def train_step(self, params, opt_state, inside, reflective, absorbing, incident):\n",
    "        \"\"\"\n",
    "        Make just one step of the training\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        opt_state : a tuple given by optax\n",
    "            -- state(hystorical) of the gradient descent\n",
    "        inside : jax.numpy.ndarray[[batch_size, batch_size,batch_size]]\n",
    "            -- (x,y,t) points from the mesh\n",
    "        bound : jax.numpy.ndarray[[batch_size, batch_size,batch_size]]\n",
    "            -- (x,y,t) points from boundary\n",
    "        initial : jax.numpy.ndarray[[batch_size, batch_size, batch_size]]\n",
    "            -- (x,y,t) points from initial condition\n",
    "        Returns\n",
    "        -------\n",
    "        loss : a float.64\n",
    "            -- loss function applied to inputs\n",
    "        new_params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias updated\n",
    "        opt_state : a tuple given by optax\n",
    "            -- update the state(hystorical) of the gradient descent\n",
    "        losses : dictionary with the keys (loss_m, loss_b, loss_i)\n",
    "            -- current values of each loss function\n",
    "        \"\"\"\n",
    "\n",
    "        (loss,losses), gradient = jax.value_and_grad(self.loss_function, has_aux=True)(params, inside, reflective, absorbing, incident)\n",
    "        updates, new_opt_state = self.optimizer.update(gradient, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "        return loss, new_params, new_opt_state, losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    #################### Physical domain ####################\n",
    "    #########################################################\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def omega_coeff(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute the frequency omega at in the couple (x,y)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        Returns\n",
    "        -------\n",
    "        omega : jax.numpy.ndarray[batch_size]\n",
    "            -- omega in each couple (x,y)\n",
    "        \"\"\"\n",
    "        omega = jax.numpy.sqrt(9.81*self.k_coeff(x,y)*jax.numpy.tanh(self.k_coeff(x,y)*self.height_function(x,y)))\n",
    "        \n",
    "        return omega\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def k_coeff(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute the coefficient k in the couple (x,y)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        Returns\n",
    "        -------\n",
    "        k : jax.numpy.ndarray[batch_size]\n",
    "            -- Coefficient k in each couple (x,y)\n",
    "        \"\"\"\n",
    "\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def c_coeff(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute the coefficient c in the couple (x,y)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        Returns\n",
    "        -------\n",
    "        c : jax.numpy.ndarray[batch_size]\n",
    "            -- Coefficient c in each couple (x,y)\n",
    "        \"\"\"\n",
    "        c = self.omega_coeff(x,y)/self.k_coeff(x,y)\n",
    "\n",
    "        return c\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def cg_coeff(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute the coefficient cg in the couple (x,y)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        Returns\n",
    "        -------\n",
    "        cg : jax.numpy.ndarray[batch_size]\n",
    "            -- Coefficient cg in each couple (x,y)\n",
    "        \"\"\"\n",
    "        nprime = 0.5*(1 + 2*self.k_coeff(x,y)*self.height_function(x,y)/jax.numpy.sinh(2*self.k_coeff(x,y)*self.height_function(x,y)))\n",
    "        cg = nprime*self.omega_coeff(x,y)/self.k_coeff(x,y)\n",
    "\n",
    "        return cg\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def height_function(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute the height in the couple (x,y)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        Returns\n",
    "        -------\n",
    "        h : jax.numpy.ndarray[batch_size]\n",
    "            -- Height each couple (x,y)\n",
    "        \"\"\"\n",
    "\n",
    "        return 10\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def f_dependence(self, inputX, inputY, inputZ):\n",
    "        \"\"\"\n",
    "        Compute the function fz in the point (x,y,z)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        z : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis z\n",
    "        Returns\n",
    "        -------\n",
    "        fz : jax.numpy.ndarray[batch_size]\n",
    "            -- fz in each point (x,y,z)\n",
    "        \"\"\"\n",
    "        fz = (jax.numpy.cosh(self.k_coeff(inputX, inputY)*(self.height_function(inputX, inputY)+ inputZ))/jax.numpy.cosh(self.k_coeff(inputX, inputY)*self.height_function(inputX, inputY))).reshape(-1)\n",
    "\n",
    "        return fz\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def solution(self, params, x,y,z,t):\n",
    "        \"\"\"\n",
    "        Compute the solution in the point (x,y,z,t)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        z : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis z\n",
    "        t : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the time\n",
    "        Returns\n",
    "        -------\n",
    "        res : jax.numpy.ndarray[batch_size]\n",
    "            -- result in each point (x,y,z,t)\n",
    "        \"\"\"\n",
    "        H = 2*self.omega_coeff(x,y)*abs(self.spatial_solution2d(params,x,y))/9.81\n",
    "        #res = jax.numpy.real(9.81*H/(2*self.omega_coeff(x,y))*self.spatial_solution2d(params, x,y)*self.f_dependence(x,y,z)*jax.numpy.exp(jax.lax.complex(0.0,-self.omega_coeff(x,y)*t)))\n",
    "        res = jax.numpy.imag(9.81*H/(2*self.omega_coeff(x,y))*self.spatial_solution2d(params, x,y)*self.f_dependence(x,y,z)*jax.numpy.exp(jax.lax.complex(0.0,jax.numpy.real(-self.omega_coeff(x,y)*t))))\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def elevation(self, params, x,y,t):\n",
    "        \"\"\"\n",
    "        Compute the surface displacement in the point (x,y,t)\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis x\n",
    "        y : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the axis y\n",
    "        t : jax.numpy.ndarray[batch_size]\n",
    "            -- points in the time\n",
    "        Returns\n",
    "        -------\n",
    "        res : jax.numpy.ndarray[batch_size]\n",
    "            -- Displacement in each point (x,y,z,t)\n",
    "        \"\"\"\n",
    "        inputs = jax.numpy.column_stack((x,y,jax.numpy.zeros_like(y),t))\n",
    "        res = -self.dsol_dt(params, inputs)/9.81\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=(0,))    \n",
    "    def incident_psi(self, params, inputs, a=0.01):\n",
    "        \"\"\"\n",
    "        Compute the incident wave in inputs points\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of parameters[[w1,b1],...,[wn,bn]]\n",
    "            -- weights and bias\n",
    "        inputs : jax.numpy.ndarray[[batch_size, batch_size]]\n",
    "            -- (x,y) points in the mesh\n",
    "        Returns\n",
    "        -------\n",
    "        incident_wave : a float.64\n",
    "            -- incident_wave applied to inputs\n",
    "        \"\"\"\n",
    "        incident_wave = (2*a*9.81/self.c_coeff(inputs[:,0], inputs[:,1])*jax.numpy.exp(-self.k_coeff(inputs[:,0],inputs[:,1])*inputs[:,1])).reshape(-1)\n",
    "        \n",
    "        return incident_wave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network parameters\n",
    "SEED = 351\n",
    "n_features, n_targets = 2, 1            # Input and output dimension \n",
    "layers = [n_features, 50, 50, 50, 50, 50, n_targets]             # Layers structure\n",
    "lr_scheduler = optax.linear_schedule(0.0005, 0.00001, transition_steps = 50, transition_begin = 5000)\n",
    "#lr_scheduler = 0.00001               # learning rate\n",
    "optimizer = optax.adam(lr_scheduler)\n",
    "\n",
    "# Training parameters\n",
    "maximum_num_epochs = 10000       \n",
    "report_steps = 1000\n",
    "options = 1 # If 1 we start a new training\n",
    "\n",
    "# Data parameters\n",
    "N_inside = 128                # number of points inside the mesh\n",
    "N_bound = 64                   # number of points at the boundary\n",
    "domain_bounds = jax.numpy.vstack(([0,11], [0,25]))      # minimal and maximal value of each axis (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = MLP(layers,training=True)     \n",
    "key1, key2, key3 = jax.random.split(jax.random.PRNGKey(SEED), 3)\n",
    "x = jax.random.uniform(key1, (10, layers[0]))\n",
    "params = training_model.init({'params': key2}, x)\n",
    "\n",
    "eval_model = MLP(layers,training=False)\n",
    "NN_eval = eval_model.apply   # Evaluate function\n",
    "\n",
    "solver = PINN(NN_eval, optimizer)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Boundary data\n",
    "## Free boundary top/bottom\n",
    "X, Y = jax.numpy.meshgrid(jax.numpy.linspace(domain_bounds[0,0],domain_bounds[0,1],N_bound//4),jax.numpy.linspace(domain_bounds[1,0],domain_bounds[1,1],2))\n",
    "X, Y = X.flatten(), Y.flatten()\n",
    "top_bottom = jax.numpy.column_stack((X,Y))\n",
    "\n",
    "## Free boundary right\n",
    "X, Y = jax.numpy.meshgrid(jax.numpy.linspace(domain_bounds[0,1],domain_bounds[0,1],1),jax.numpy.linspace(domain_bounds[1,0],domain_bounds[1,1],N_bound//4))\n",
    "X, Y = X.flatten(), Y.flatten()\n",
    "right = jax.numpy.column_stack((X,Y))\n",
    "\n",
    "## Breakwater left\n",
    "X, Y = jax.numpy.meshgrid(jax.numpy.linspace(domain_bounds[0,0],domain_bounds[0,0],1),jax.numpy.concatenate((jax.numpy.linspace(0,10,N_bound//10), jax.numpy.linspace(15,25,N_bound//10)), axis=None))\n",
    "X, Y = X.flatten(), Y.flatten()\n",
    "XY_left = jax.numpy.column_stack((X,Y))\n",
    "\n",
    "## Breakwater opening\n",
    "X, Y = jax.numpy.meshgrid(jax.numpy.linspace(domain_bounds[0,0],domain_bounds[0,0],1),jax.numpy.linspace(10,15,N_bound//10))\n",
    "X, Y = X.flatten(), Y.flatten()\n",
    "XY_left_open = jax.numpy.column_stack((X,Y))\n",
    "\n",
    "XY_bound_absorbing = jax.numpy.concatenate((right, top_bottom))\n",
    "\n",
    "#### Inside data\n",
    "# ran_key, batch_key = jax.random.split(jax.random.PRNGKey(0))\n",
    "# XY_inside = jax.random.uniform(batch_key, shape=(N_inside, n_features-1), minval=domain_bounds[:2,0], maxval=domain_bounds[:2,1])\n",
    "\n",
    "x = jax.numpy.linspace(domain_bounds[0,0]+5/int(jax.numpy.sqrt(N_inside)),domain_bounds[0,1]-5/int(jax.numpy.sqrt(N_inside)), int(jax.numpy.sqrt(N_inside)))\n",
    "y = jax.numpy.linspace(domain_bounds[1,0]+5/int(jax.numpy.sqrt(N_inside)),domain_bounds[1,1]-5/int(jax.numpy.sqrt(N_inside)),int(jax.numpy.sqrt(N_inside)))\n",
    "x, y = jax.numpy.meshgrid(x,y)\n",
    "x, y = x.flatten(), y.flatten()\n",
    "XY_inside = jax.numpy.column_stack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points on the boundary: 66\n",
      "Number of points inside the domain: 121\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "title = ax.set_title('Spatial domain - 2d')\n",
    "graph = matplotlib.pyplot.scatter(XY_bound_absorbing[:,0],XY_bound_absorbing[:,1],color='red',s=5)\n",
    "graph = matplotlib.pyplot.scatter(XY_left_open[:,0],XY_left_open[:,1],color='green',s=5)\n",
    "graph = matplotlib.pyplot.scatter(XY_left[:,0],XY_left[:,1],color='black',s=5)\n",
    "graph = matplotlib.pyplot.scatter(XY_inside[:,0],XY_inside[:,1], color='gray', s=5)\n",
    "__ = ax.legend(['Flow-out','Flow-in','Reflective','Inside'])\n",
    "print('Number of points on the boundary:', XY_bound_absorbing.shape[0]+XY_left_open.shape[0]+XY_left.shape[0])\n",
    "print('Number of points inside the domain:', XY_inside.shape[0])\n",
    "\n",
    "matplotlib.pyplot.savefig('./images/domain2d.png', facecolor='white', bbox_inches = 'tight')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad requires real-valued outputs (output dtype that is a sub-dtype of np.floating), but got complex128. For holomorphic differentiation, pass holomorphic=True. For differentiation of non-holomorphic functions involving complex outputs, use jax.vjp directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Main loop to solve the PDE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m ibatch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(maximum_num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss, params, opt_state, losses \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mtrain_step(params, opt_state, XY_inside, XY_left, XY_bound_absorbing, XY_left_open)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_i\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(losses[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss_b\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(losses[\u001b[39m1\u001b[39m]))\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "\u001b[1;32m/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb Cell 17\u001b[0m in \u001b[0;36mPINN.train_step\u001b[0;34m(self, params, opt_state, inside, reflective, absorbing, incident)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mpartial(jax\u001b[39m.\u001b[39mjit, static_argnums\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,))    \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, params, opt_state, inside, reflective, absorbing, incident):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m    Make just one step of the training\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39m        -- current values of each loss function\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m     (loss,losses), gradient \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mvalue_and_grad(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_function, has_aux\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(params, inside, reflective, absorbing, incident)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     updates, new_opt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mupdate(gradient, opt_state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/stardorse/Documents/Tsunami/src/PINN_V2/mild_slope/mild_slope.ipynb#X22sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m     new_params \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/api.py:1210\u001b[0m, in \u001b[0;36m_check_output_dtype_revderiv\u001b[0;34m(name, holomorphic, x)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m with holomorphic=True requires outputs with complex dtype, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00maval\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1209\u001b[0m \u001b[39melif\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mcomplexfloating):\n\u001b[0;32m-> 1210\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m requires real-valued outputs (output dtype that is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1211\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ma sub-dtype of np.floating), but got \u001b[39m\u001b[39m{\u001b[39;00maval\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1212\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mFor holomorphic differentiation, pass holomorphic=True. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mFor differentiation of non-holomorphic functions involving complex \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39moutputs, use jax.vjp directly.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1215\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[1;32m   1216\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m requires real-valued outputs (output dtype that is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ma sub-dtype of np.floating), but got \u001b[39m\u001b[39m{\u001b[39;00maval\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1218\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mFor differentiation of functions with integer outputs, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1219\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mjax.vjp directly.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: grad requires real-valued outputs (output dtype that is a sub-dtype of np.floating), but got complex128. For holomorphic differentiation, pass holomorphic=True. For differentiation of non-holomorphic functions involving complex outputs, use jax.vjp directly."
     ]
    }
   ],
   "source": [
    "if options == 1:            # begin a new training\n",
    "    loss_history = []\n",
    "    loss_i = []               # residual loss\n",
    "    loss_b = []               # boundary loss\n",
    "\n",
    "    # Main loop to solve the PDE\n",
    "    for ibatch in range(maximum_num_epochs+1):\n",
    "        loss, params, opt_state, losses = solver.train_step(params, opt_state, XY_inside, XY_left, XY_bound_absorbing, XY_left_open)\n",
    "\n",
    "        loss_i.append(float(losses[0]))\n",
    "        loss_b.append(float(losses[1]))\n",
    "        losssum = jax.numpy.sum(losses)\n",
    "        loss_history.append(float(losssum))\n",
    "\n",
    "        if ibatch%report_steps == report_steps-1:\n",
    "            print(\"Epoch n{}: \".format(ibatch+1), loss.item())\n",
    "\n",
    "        if losssum<=numpy.min(loss_history): # save if the current state is the best \n",
    "                pickle.dump(params, open(\"./NN_saves/params_checkpoint_mild_slope\", \"wb\"))\n",
    "                pickle.dump(opt_state, open(\"./NN_saves/opt_state_checkpoint_mild_slope\", \"wb\"))\n",
    "                pickle.dump(loss_history, open(\"./NN_saves/loss_history_mild_slope\", \"wb\"))\n",
    "                pickle.dump(loss_i, open(\"./NN_saves/loss_i_mild_slope\", \"wb\"))\n",
    "                pickle.dump(loss_b, open(\"./NN_saves/loss_b_mild_slope\", \"wb\"))\n",
    "\n",
    "elif options == 2:   # continue the last training\n",
    "    params = pickle.load(open(\"./NN_saves/params_checkpoint_mild_slope\", \"rb\"))\n",
    "    opt_state = pickle.load(open(\"./NN_saves/opt_state_checkpoint_mild_slope\", \"rb\"))\n",
    "    loss_history = pickle.load(open(\"./NN_saves/loss_history_mild_slope\", \"rb\"))\n",
    "    loss_i = pickle.load(open(\"./NN_saves/loss_i_mild_slope\", \"rb\"))\n",
    "    loss_b = pickle.load(open(\"./NN_saves/loss_b_mild_slope\", \"rb\"))\n",
    "    iepoch = len(loss_history)\n",
    "\n",
    "    # Main loop to solve the PDE\n",
    "    for ibatch in range(iepoch, maximum_num_epochs+1):\n",
    "        loss, params, opt_state, losses = solver.train_step(params,opt_state, XY_inside, XY_left, XY_bound_absorbing, XY_left_open)\n",
    "\n",
    "        loss_i.append(float(losses[0]))\n",
    "        loss_b.append(float(losses[1]))\n",
    "        losssum = jax.numpy.sum(losses)\n",
    "        loss_history.append(float(losssum))\n",
    "\n",
    "        if ibatch%report_steps==report_steps-1:\n",
    "            print(\"Epoch n{}: \".format(ibatch+1), loss.item())\n",
    "\n",
    "        if losssum<=numpy.min(loss_history): # save if the current state is the best \n",
    "                pickle.dump(params, open(\"./NN_saves/params_checkpoint_mild_slope\", \"wb\"))\n",
    "                pickle.dump(opt_state, open(\"./NN_saves/opt_state_checkpoint_mild_slope\", \"wb\"))\n",
    "                pickle.dump(loss_history, open(\"./NN_saves/loss_history_mild_slope\", \"wb\"))\n",
    "                pickle.dump(loss_i, open(\"./NN_saves/loss_i_mild_slope\", \"wb\"))\n",
    "                pickle.dump(loss_b, open(\"./NN_saves/loss_b_mild_slope\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots(1, 1)\n",
    "__ = ax.plot(numpy.log10(loss_history))\n",
    "__ = ax.plot(numpy.log10(numpy.array(loss_i)))\n",
    "__ = ax.plot(numpy.log10(numpy.array(loss_b)))\n",
    "xlabel = ax.set_xlabel(r'${\\rm Step}$')\n",
    "ylabel = ax.set_ylabel(r'$\\log_{10}{\\rm (loss)}$')\n",
    "title = ax.set_title(r'${\\rm Training}$')\n",
    "ax.legend(['loss_sum','residual','boundary','initial_cond'])\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints = 1000\n",
    "values = numpy.zeros((npoints,npoints))\n",
    "\n",
    "x, y = numpy.meshgrid(numpy.linspace(domain_bounds[0,0], domain_bounds[0,1], npoints), numpy.linspace(domain_bounds[1,0], domain_bounds[1,1], npoints))\n",
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "title = ax.set_title('Mild slope equation 2d')\n",
    "\n",
    "for i in range(npoints):\n",
    "    print(\"Plotting: {} out of {}\".format(i+1, npoints), end='\\r')\n",
    "    values[i,:] = solver.solution(params, x[i,:], y[i,:])[:,0]\n",
    "\n",
    "graph = matplotlib.pyplot.pcolormesh(x, y, values, cmap = 'rainbow')\n",
    "matplotlib.pyplot.colorbar()\n",
    "matplotlib.pyplot.savefig('mild_slope.png', bbox_inches = 'tight')\n",
    "matplotlib.pyplot.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
