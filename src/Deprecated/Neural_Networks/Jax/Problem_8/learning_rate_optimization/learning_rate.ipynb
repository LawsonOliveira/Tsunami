{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate - Problem 8\n",
    "## Description\n",
    "\n",
    "### Average time : 200 minutes\n",
    "\n",
    "### PDE\n",
    "We will try to find the best learning rate to the problem 8 of the article: https://ieeexplore.ieee.org/document/712178  \n",
    "\n",
    "$\\Delta \\psi(x,y) +\\psi(x,y)\\cdot\\frac{\\partial \\psi(x,y)}{\\partial y}= f(x,y)$ on $\\Omega = [0,1]^2$  \n",
    "where $f(x, y)=\\sin(\\pi x)(2-\\pi^2y^2+2y^3\\sin(\\pi x))$   \n",
    "\n",
    "### Boundary conditions    \n",
    "$\\psi(0,y)=\\psi(1,y)=\\psi(x,0)=0$ and $\\frac{\\partial \\psi}{\\partial y}(x,1)=2\\sin(\\pi x)$           \n",
    "\n",
    "### Loss function\n",
    "The loss to minimize here is $\\mathcal{L} = ||\\Delta \\psi(x,y) +\\psi(x,y)\\cdot\\frac{\\partial \\psi(x,y)}{\\partial y}-f(x,y) ||_2$  \n",
    "\n",
    "### Analytical solution\n",
    "The true function $\\psi$ should be $\\psi(x, y)=y^2sin(\\pi x)$  \n",
    "This solution is the same of the problem 7\n",
    "\n",
    "### Approximated solution\n",
    "We want find a solution $\\psi(x,y)=A(x,y)+F(x,y)N(x,y)$\n",
    "s.t:  \n",
    "$F(x,y)=\\sin(x-1)\\sin(y-1)\\sin(x)\\sin(y)$ \n",
    "$A(x,y)=y\\sin(\\pi x)$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "# Jax libraries\n",
    "from jax import value_and_grad,vmap,jit,jacfwd\n",
    "from functools import partial \n",
    "from jax import random as jran\n",
    "from jax.example_libraries import optimizers as jax_opt\n",
    "from jax.nn import tanh, sigmoid, elu, relu, gelu\n",
    "from jax.lib import xla_bridge\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Others libraries\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "        Create a multilayer perceptron and initialize the neural network\n",
    "    Inputs :\n",
    "        A SEED number and the layers structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class initialization\n",
    "    def __init__(self,SEED,layers):\n",
    "        self.key=jran.PRNGKey(SEED)\n",
    "        self.keys = jran.split(self.key,len(layers))\n",
    "        self.layers=layers\n",
    "        self.params = []\n",
    "\n",
    "    # Initialize the MLP weigths and bias\n",
    "    def MLP_create(self):\n",
    "        for layer in range(0, len(self.layers)-1):\n",
    "            in_size,out_size=self.layers[layer], self.layers[layer+1]\n",
    "            std_dev = jnp.sqrt(2/(in_size + out_size ))\n",
    "            weights=jran.truncated_normal(self.keys[layer], -2, 2, shape=(out_size, in_size), dtype=np.float32)*std_dev\n",
    "            bias=jran.truncated_normal(self.keys[layer], -1, 1, shape=(out_size, 1), dtype=np.float32).reshape((out_size,))\n",
    "            self.params.append((weights,bias))\n",
    "        return self.params\n",
    "        \n",
    "    # Evaluate a position XY using the neural network    \n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def NN_evaluation(self,new_params, inputs):\n",
    "        for layer in range(0, len(new_params)-1):\n",
    "            weights, bias = new_params[layer]\n",
    "            inputs = gelu(jnp.add(jnp.dot(inputs, weights.T), bias))\n",
    "        weights, bias = new_params[-1]\n",
    "        output = jnp.dot(inputs, weights.T)+bias\n",
    "        return output\n",
    "    \n",
    "    # Get the key associated with the neural network\n",
    "    def get_key(self):\n",
    "        return self.key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two dimensional PDE operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDE_operators2d:\n",
    "    \"\"\"\n",
    "        Class with the most common operators used to solve PDEs\n",
    "    Input:\n",
    "        A function that we want to compute the respective operator\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class initialization\n",
    "    def __init__(self,function):\n",
    "        self.function=function\n",
    "\n",
    "    # Compute the two dimensional laplacian\n",
    "    def laplacian_2d(self,params,inputs):\n",
    "        fun = lambda params,x,y: self.function(params, x,y)\n",
    "        @partial(jit)    \n",
    "        def action(params,x,y):\n",
    "            u_xx = jacfwd(jacfwd(fun, 1), 1)(params,x,y)\n",
    "            u_yy = jacfwd(jacfwd(fun, 2), 2)(params,x,y)\n",
    "            return u_xx + u_yy\n",
    "        vec_fun = vmap(action, in_axes = (None, 0, 0))\n",
    "        laplacian = vec_fun(params, inputs[:,0], inputs[:,1])\n",
    "        return laplacian\n",
    "\n",
    "    # Compute the partial derivative in x\n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def du_dx(self,params,inputs):\n",
    "        fun = lambda params,x,y: self.function(params, x,y)\n",
    "        @partial(jit)    \n",
    "        def action(params,x,y):\n",
    "            u_x = jacfwd(fun, 1)(params,x,y)\n",
    "            return u_x\n",
    "        vec_fun = vmap(action, in_axes = (None, 0, 0))\n",
    "        return vec_fun(params, inputs[:,0], inputs[:,1])\n",
    "\n",
    "    # Compute the partial derivative in y\n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def du_dy(self,params,inputs):\n",
    "        fun = lambda params,x,y: self.function(params, x,y)\n",
    "        @partial(jit)    \n",
    "        def action(params,x,y):\n",
    "            u_y = jacfwd(fun, 2)(params,x,y)\n",
    "            return u_y\n",
    "        vec_fun = vmap(action, in_axes = (None, 0, 0))\n",
    "        return vec_fun(params, inputs[:,0], inputs[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    \"\"\"\n",
    "    Solve a PDE using Physics Informed Neural Networks   \n",
    "    Input:\n",
    "        The evaluation function of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # Class initialization\n",
    "    def __init__(self,NN_evaluation):\n",
    "        self.operators=PDE_operators2d(self.solution)\n",
    "        self.laplacian=self.operators.laplacian_2d\n",
    "        self.NN_evaluation=NN_evaluation\n",
    "        self.dsol_dy=self.operators.du_dy\n",
    "\n",
    "    # Definition of the function A(x,y) mentioned above\n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def A_function(self,inputX,inputY):\n",
    "        return jnp.multiply(inputY,jnp.sin(jnp.pi*inputX)).reshape(-1,1)\n",
    "\n",
    "    # Definition of the function F(x,y) mentioned above   \n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def F_function(self,inputX,inputY):\n",
    "        F1=jnp.multiply(jnp.sin(inputX),jnp.sin(inputX-jnp.ones_like(inputX)))\n",
    "        F2=jnp.multiply(jnp.sin(inputY),jnp.sin(inputY-jnp.ones_like(inputY)))\n",
    "        return jnp.multiply(F1,F2).reshape((-1,1))\n",
    "\n",
    "    # Definition of the function f(x,y) mentioned above   \n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def target_function(self,inputs):\n",
    "        return jnp.multiply(jnp.sin(jnp.pi*inputs[:,0]),2-jnp.pi**2*inputs[:,1]**2+2*inputs[:,1]**3*jnp.sin(jnp.pi*inputs[:,0])).reshape(-1,1) \n",
    "\n",
    "    # Compute the solution of the PDE on the points (x,y)\n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def solution(self,params,inputX,inputY):\n",
    "        inputs=jnp.column_stack((inputX,inputY))\n",
    "        NN = vmap(partial(jit(self.NN_evaluation), params))(inputs)\n",
    "        F=self.F_function(inputX,inputY)\n",
    "        A=self.A_function(inputX,inputY)\n",
    "        return jnp.add(jnp.multiply(F,NN),A).reshape(-1,1)\n",
    "\n",
    "    # Compute the loss function\n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def loss_function(self,params,batch):\n",
    "        targets=self.target_function(batch)\n",
    "        laplacian=self.laplacian(params,batch).reshape(-1,1)\n",
    "        dsol_dy_values=self.dsol_dy(params,batch)[:,0].reshape((-1,1))\n",
    "        preds=laplacian+jnp.multiply(self.solution(params,batch[:,0],batch[:,1]),dsol_dy_values).reshape(-1,1)\n",
    "        return jnp.linalg.norm(preds-targets)\n",
    " \n",
    "    # Train step\n",
    "    @partial(jit, static_argnums=(0,))    \n",
    "    def train_step(self,i, opt_state, inputs):\n",
    "        params = get_params(opt_state)\n",
    "        loss, gradient = value_and_grad(self.loss_function)(params,inputs)\n",
    "        return loss, opt_update(i, gradient, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network parameters\n",
    "SEED = 351\n",
    "n_features, n_targets = 2, 1            # Input and output dimension\n",
    "layers = [n_features,30,n_targets]      # Layers structure\n",
    "\n",
    "# Initialization\n",
    "NN_MLP=MLP(SEED,layers)                 \n",
    "params = NN_MLP.MLP_create()            # Create the MLP\n",
    "NN_eval=NN_MLP.NN_evaluation            # Evaluate function\n",
    "solver=PINN(NN_eval)\n",
    "key=NN_MLP.get_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_batches = 100000\n",
    "report_steps=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "[4.0000000e-07 7.9999961e-07 1.1999998e-06 1.6000000e-06 1.9999995e-06\n",
      " 2.3999999e-06 2.7999999e-06 3.1999996e-06 3.5999997e-06 4.0000000e-06\n",
      " 4.3999994e-06 4.7999997e-06 5.2000000e-06 5.5999994e-06 5.9999998e-06\n",
      " 6.3999996e-06 6.7999995e-06 7.1999998e-06 7.5999997e-06 7.9999954e-06\n",
      " 7.9999991e-06 8.3999994e-06 8.7999997e-06 9.2000000e-06 9.5999994e-06\n",
      " 9.9999997e-06 1.1999997e-05 1.6000000e-05 1.9999994e-05 2.3999997e-05\n",
      " 2.7999999e-05 3.1999996e-05 3.5999998e-05 3.9999999e-05 4.3999997e-05\n",
      " 4.7999998e-05 5.1999999e-05 5.5999993e-05 5.9999998e-05 6.3999993e-05\n",
      " 6.7999994e-05 7.1999995e-05 7.5999997e-05 7.9999962e-05 7.9999991e-05\n",
      " 8.3999999e-05 8.7999993e-05 9.2000002e-05 9.5999989e-05 9.9999997e-05\n",
      " 1.1999998e-04 1.6000001e-04 1.9999997e-04 2.3999999e-04 2.8000001e-04\n",
      " 3.1999996e-04 3.6000001e-04 3.9999999e-04 4.3999997e-04 4.7999999e-04\n",
      " 5.2000000e-04 5.5999996e-04 5.9999997e-04 6.4000004e-04 6.8000000e-04\n",
      " 7.2000001e-04 7.6000002e-04 7.9999957e-04 8.0000004e-04 8.3999999e-04\n",
      " 8.8000001e-04 9.2000008e-04 9.6000003e-04 1.0000000e-03 1.1999998e-03\n",
      " 1.6000000e-03 1.9999996e-03 2.3999996e-03 2.7999999e-03 3.1999995e-03\n",
      " 3.5999997e-03 3.9999997e-03 4.0000002e-03 4.3999995e-03 4.7999998e-03\n",
      " 5.2000000e-03 5.5999993e-03 5.9999996e-03 6.3999998e-03 6.7999992e-03\n",
      " 7.1999999e-03 7.5999997e-03 7.9999967e-03 7.9999985e-03 8.3999997e-03\n",
      " 8.8000000e-03 9.1999993e-03 9.5999995e-03 9.9999998e-03 1.1999999e-02\n",
      " 1.6000001e-02 1.9999998e-02 2.3999998e-02 2.8000001e-02 3.1999998e-02\n",
      " 3.5999998e-02 3.9999999e-02 4.0000003e-02 4.3999996e-02 4.7999997e-02\n",
      " 5.2000001e-02 5.5999998e-02 5.9999999e-02 6.4000003e-02 6.7999996e-02\n",
      " 7.1999997e-02 7.6000005e-02 7.9999961e-02 7.9999998e-02 8.3999999e-02\n",
      " 8.8000000e-02 9.2000000e-02 9.6000001e-02 1.0000000e-01 1.1999998e-01\n",
      " 1.6000000e-01 1.9999996e-01 2.3999998e-01 2.8000000e-01 3.1999996e-01\n",
      " 3.5999998e-01 4.0000001e-01 4.3999997e-01 4.7999999e-01 5.1999998e-01\n",
      " 5.5999994e-01 5.9999996e-01 6.3999999e-01 6.7999995e-01 7.1999997e-01\n",
      " 7.5999999e-01 7.9999995e-01 8.3999997e-01 8.8000000e-01 9.2000002e-01\n",
      " 9.5999998e-01 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "init, end, interval_lenght = 0, 6, 25\n",
    "# Learning rate values\n",
    "intervals = jnp.array([jnp.linspace(10**(-i),10**(-i)/interval_lenght,interval_lenght) for i in range(init,end)])\n",
    "learning_rate = jnp.unique(jnp.sort(intervals.reshape(-1,1)[:,0]))\n",
    "print(len(learning_rate))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 99\n",
      "loss = 0.0056283138692379 learning rate = 9.599999e-05\n",
      "minimum value = 0.0056283138692379\n",
      "iteration 2 of 99\n",
      "loss = 0.0048086014576256275 learning rate = 1e-04\n",
      "minimum value = 0.0048086014576256275\n",
      "iteration 3 of 99\n",
      "loss = 0.00437261164188385 learning rate = 0.00011999998\n",
      "minimum value = 0.00437261164188385\n",
      "iteration 4 of 99\n",
      "loss = 0.00505151366814971 learning rate = 0.00016000001\n",
      "iteration 5 of 99\n",
      "loss = 0.0052724299021065235 learning rate = 0.00019999997\n",
      "iteration 6 of 99\n",
      "loss = 0.005549023859202862 learning rate = 0.00024\n",
      "iteration 7 of 99\n",
      "loss = 0.005764973349869251 learning rate = 0.00028\n",
      "iteration 8 of 99\n",
      "loss = 0.005953838117420673 learning rate = 0.00031999996\n",
      "iteration 9 of 99\n",
      "loss = 0.006143741775304079 learning rate = 0.00036\n",
      "iteration 10 of 99\n",
      "loss = 0.006300668697804213 learning rate = 0.0004\n",
      "iteration 11 of 99\n",
      "loss = 0.0064275446347892284 learning rate = 0.00043999997\n",
      "iteration 12 of 99\n",
      "loss = 0.0065496391616761684 learning rate = 0.00048\n",
      "iteration 13 of 99\n",
      "loss = 0.006680157035589218 learning rate = 0.00052\n",
      "iteration 14 of 99\n",
      "loss = 0.006793366279453039 learning rate = 0.00055999996\n",
      "iteration 15 of 99\n",
      "loss = 0.006896598730236292 learning rate = 0.00059999997\n",
      "iteration 16 of 99\n",
      "loss = 0.0069457595236599445 learning rate = 0.00064000004\n",
      "iteration 17 of 99\n",
      "loss = 0.007037411909550428 learning rate = 0.00068\n",
      "iteration 18 of 99\n",
      "loss = 0.007094191387295723 learning rate = 0.00072\n",
      "iteration 19 of 99\n",
      "loss = 0.00715602608397603 learning rate = 0.00076\n",
      "iteration 20 of 99\n",
      "loss = 0.007218529935926199 learning rate = 0.0007999996\n",
      "iteration 21 of 99\n",
      "loss = 0.007215419318526983 learning rate = 0.00080000004\n",
      "iteration 22 of 99\n",
      "loss = 0.007279032841324806 learning rate = 0.00084\n",
      "iteration 23 of 99\n",
      "loss = 0.00733697647228837 learning rate = 0.00088\n",
      "iteration 24 of 99\n",
      "loss = 0.007450229488313198 learning rate = 0.0009200001\n",
      "iteration 25 of 99\n",
      "loss = 0.007429111283272505 learning rate = 0.00096000003\n",
      "iteration 26 of 99\n",
      "loss = 0.007506238296627998 learning rate = 0.001\n",
      "iteration 27 of 99\n",
      "loss = 0.007746902294456959 learning rate = 0.0011999998\n",
      "iteration 28 of 99\n",
      "loss = 0.008289686404168606 learning rate = 0.0016\n",
      "iteration 29 of 99\n",
      "loss = 0.008984302170574665 learning rate = 0.0019999996\n",
      "iteration 30 of 99\n",
      "loss = 0.009617257863283157 learning rate = 0.0023999996\n",
      "iteration 31 of 99\n",
      "loss = 0.01011795923113823 learning rate = 0.0028\n",
      "iteration 32 of 99\n",
      "loss = 0.0107788797467947 learning rate = 0.0031999995\n",
      "iteration 33 of 99\n",
      "loss = 0.011304347775876522 learning rate = 0.0035999997\n",
      "iteration 34 of 99\n",
      "loss = 0.0116304662078619 learning rate = 0.0039999997\n",
      "iteration 35 of 99\n",
      "loss = 0.011619005352258682 learning rate = 0.004\n",
      "iteration 36 of 99\n",
      "loss = 0.011790497228503227 learning rate = 0.0043999995\n",
      "iteration 37 of 99\n",
      "loss = 0.011702973395586014 learning rate = 0.0047999998\n",
      "iteration 38 of 99\n",
      "loss = 0.011884546838700771 learning rate = 0.0052\n",
      "iteration 39 of 99\n",
      "loss = 0.012837653048336506 learning rate = 0.0055999993\n",
      "iteration 40 of 99\n",
      "loss = 0.012833688408136368 learning rate = 0.0059999996\n",
      "iteration 41 of 99\n",
      "loss = 0.013390054926276207 learning rate = 0.0064\n",
      "iteration 42 of 99\n",
      "loss = 0.013657047413289547 learning rate = 0.006799999\n",
      "iteration 43 of 99\n",
      "loss = 0.013788939453661442 learning rate = 0.0072\n",
      "iteration 44 of 99\n",
      "loss = 0.013915683142840862 learning rate = 0.0075999997\n",
      "iteration 45 of 99\n",
      "loss = 0.014332626946270466 learning rate = 0.007999997\n",
      "iteration 46 of 99\n",
      "loss = 0.014518260024487972 learning rate = 0.0079999985\n",
      "iteration 47 of 99\n",
      "loss = 0.014350385405123234 learning rate = 0.0084\n",
      "iteration 48 of 99\n",
      "loss = 0.014661336317658424 learning rate = 0.0088\n",
      "iteration 49 of 99\n",
      "loss = 0.015009942464530468 learning rate = 0.009199999\n",
      "iteration 50 of 99\n",
      "loss = 0.015280765481293201 learning rate = 0.0095999995\n",
      "iteration 51 of 99\n",
      "loss = 0.015574362128973007 learning rate = 0.01\n",
      "iteration 52 of 99\n",
      "loss = 0.015930278226733208 learning rate = 0.011999999\n",
      "iteration 53 of 99\n",
      "loss = 0.018411044031381607 learning rate = 0.016\n",
      "iteration 54 of 99\n",
      "loss = 0.02647589147090912 learning rate = 0.019999998\n",
      "iteration 55 of 99\n",
      "loss = 0.026135995984077454 learning rate = 0.023999998\n",
      "iteration 56 of 99\n",
      "loss = 0.02557310089468956 learning rate = 0.028\n",
      "iteration 57 of 99\n",
      "loss = 0.01927211880683899 learning rate = 0.031999998\n",
      "iteration 58 of 99\n",
      "loss = 0.025239434093236923 learning rate = 0.036\n",
      "iteration 59 of 99\n",
      "loss = 0.042144015431404114 learning rate = 0.04\n",
      "iteration 60 of 99\n",
      "loss = 0.03771758824586868 learning rate = 0.040000003\n",
      "iteration 61 of 99\n",
      "loss = 0.03965884819626808 learning rate = 0.043999996\n",
      "iteration 62 of 99\n",
      "loss = 0.02181864343583584 learning rate = 0.047999997\n",
      "iteration 63 of 99\n",
      "loss = 0.04423406347632408 learning rate = 0.052\n",
      "iteration 64 of 99\n",
      "loss = 0.05317981541156769 learning rate = 0.055999998\n",
      "iteration 65 of 99\n",
      "loss = 0.042907778173685074 learning rate = 0.06\n",
      "iteration 66 of 99\n",
      "loss = 0.05452097952365875 learning rate = 0.064\n",
      "iteration 67 of 99\n",
      "loss = 0.04155665636062622 learning rate = 0.067999996\n",
      "iteration 68 of 99\n",
      "loss = 0.04972462356090546 learning rate = 0.072\n",
      "iteration 69 of 99\n",
      "loss = 0.04367397353053093 learning rate = 0.076000005\n",
      "iteration 70 of 99\n",
      "loss = 0.07090074568986893 learning rate = 0.07999996\n",
      "iteration 71 of 99\n",
      "loss = 0.042590875178575516 learning rate = 0.08\n",
      "iteration 72 of 99\n",
      "loss = 0.08229882270097733 learning rate = 0.084\n",
      "iteration 73 of 99\n",
      "loss = 0.06757473945617676 learning rate = 0.088\n",
      "iteration 74 of 99\n",
      "loss = 0.09713171422481537 learning rate = 0.092\n",
      "iteration 75 of 99\n",
      "loss = 0.07808905094861984 learning rate = 0.096\n",
      "iteration 76 of 99\n",
      "loss = 0.08564911037683487 learning rate = 0.1\n",
      "iteration 77 of 99\n",
      "loss = 0.11132657527923584 learning rate = 0.11999998\n",
      "iteration 78 of 99\n",
      "loss = 0.11153824627399445 learning rate = 0.16\n",
      "iteration 79 of 99\n",
      "loss = 0.14020493626594543 learning rate = 0.19999996\n",
      "iteration 80 of 99\n",
      "loss = 0.22732415795326233 learning rate = 0.23999998\n",
      "iteration 81 of 99\n",
      "loss = 0.11888231337070465 learning rate = 0.28\n",
      "iteration 82 of 99\n",
      "loss = 0.12719812989234924 learning rate = 0.31999996\n",
      "iteration 83 of 99\n",
      "loss = 0.33327990770339966 learning rate = 0.35999998\n",
      "iteration 84 of 99\n",
      "loss = 0.18740606307983398 learning rate = 0.4\n",
      "iteration 85 of 99\n",
      "loss = 0.36471953988075256 learning rate = 0.43999997\n",
      "iteration 86 of 99\n",
      "loss = 0.7714767456054688 learning rate = 0.48\n",
      "iteration 87 of 99\n",
      "loss = 0.6909691095352173 learning rate = 0.52\n",
      "iteration 88 of 99\n",
      "loss = 0.7433273196220398 learning rate = 0.55999994\n",
      "iteration 89 of 99\n",
      "loss = 0.439883828163147 learning rate = 0.59999996\n",
      "iteration 90 of 99\n",
      "loss = 0.9097825288772583 learning rate = 0.64\n",
      "iteration 91 of 99\n",
      "loss = 0.6054460406303406 learning rate = 0.67999995\n",
      "iteration 92 of 99\n",
      "loss = 0.6783227920532227 learning rate = 0.71999997\n",
      "iteration 93 of 99\n",
      "loss = 0.6899469494819641 learning rate = 0.76\n",
      "iteration 94 of 99\n",
      "loss = 2.1728146076202393 learning rate = 0.79999995\n",
      "iteration 95 of 99\n",
      "loss = 1.0038481950759888 learning rate = 0.84\n",
      "iteration 96 of 99\n",
      "loss = 1.763105869293213 learning rate = 0.88\n",
      "iteration 97 of 99\n",
      "loss = 2.2033491134643555 learning rate = 0.92\n",
      "iteration 98 of 99\n",
      "loss = 2.211721897125244 learning rate = 0.96\n",
      "iteration 99 of 99\n",
      "loss = 3.3589024543762207 learning rate = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Main loop find the best learning rate\n",
    "counter=0\n",
    "min_index=jnp.inf\n",
    "min_loss_value = jnp.inf\n",
    "minimum_loss=[]\n",
    "\n",
    "# Create a file to save the learning rate\n",
    "file_data_learn=open('./learning_rate','w')\n",
    "file_data_learn.close()\n",
    "\n",
    "# Create a file to save the last value of the loss function\n",
    "file_data_loss=open('./loss_function','w')\n",
    "file_data_loss.close()\n",
    "\n",
    "for i in range(len(learning_rate)):\n",
    "    loss_history = []\n",
    "    opt_init, opt_update, get_params = jax_opt.adam(learning_rate[i])\n",
    "\n",
    "    NN_MLP=MLP(SEED,layers)                 \n",
    "    params = NN_MLP.MLP_create()            # Create the MLP\n",
    "    NN_eval=NN_MLP.NN_evaluation            # Evaluate function\n",
    "    solver=PINN(NN_eval)                    # Use PINN on the problem 8\n",
    "    key=NN_MLP.get_key()                    # Get the key of NN\n",
    "\n",
    "    opt_state = opt_init(params)            # Initialize opt_state\n",
    "    \n",
    "    for ibatch in range(0,num_batches):\n",
    "        ran_key, batch_key = jran.split(key)\n",
    "        XY_train = jran.uniform(batch_key, shape=(batch_size, n_features), minval=0, maxval=1)\n",
    "\n",
    "        loss, opt_state = solver.train_step(ibatch,opt_state, XY_train)\n",
    "        loss_history.append(float(loss))\n",
    "\n",
    "        #if ibatch%report_steps==report_steps-1:\n",
    "            #print(\"Epoch nÂ°{}: \".format(ibatch+1), loss.item())\n",
    "\n",
    "    print(\"iteration\",i+1,\"of\",len(learning_rate))\n",
    "    print(\"loss =\",loss_history[num_batches-1],\"learning rate =\",learning_rate[i])\n",
    "    minimum_loss.append(loss_history[num_batches-1])\n",
    "\n",
    "    # Get the index for the best learning rate\n",
    "    if loss_history[num_batches-1]<min_loss_value:\n",
    "        min_loss_value = loss_history[num_batches-1]\n",
    "        min_index=i\n",
    "        print('minimum value =',minimum_loss[i])\n",
    "\n",
    "    # Save the learning rate\n",
    "    file_data_learn=open('./learning_rate','a')\n",
    "    file_data_learn.write(str(learning_rate[i])+',')\n",
    "    file_data_learn.close()\n",
    "\n",
    "    # Save the last value of the loss function\n",
    "    file_data_loss=open('./loss_function','a')\n",
    "    file_data_loss.write(str(loss_history[num_batches-1])+',')\n",
    "    file_data_loss.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot learning rate optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning rate = 0.00011999998 \n",
      "loss = 0.00437261164188385\n"
     ]
    }
   ],
   "source": [
    "min_index=50\n",
    "learning_rate=[4e-07,7.999996e-07,1.1999998e-06,1.6e-06,1.9999995e-06,2.3999999e-06,2.8e-06,3.1999996e-06,3.5999997e-06,4e-06,4.3999994e-06,4.7999997e-06,5.2e-06,5.5999994e-06,5.9999998e-06,6.3999996e-06,6.7999995e-06,7.2e-06,7.5999997e-06,7.999995e-06,7.999999e-06,8.399999e-06,8.8e-06,9.2e-06,9.599999e-06,1e-05,1.1999997e-05,1.6e-05,1.9999994e-05,2.3999997e-05,2.7999999e-05,3.1999996e-05,3.5999998e-05,4e-05,4.3999997e-05,4.7999998e-05,5.2e-05,5.5999993e-05,6e-05,6.399999e-05,6.7999994e-05,7.1999995e-05,7.6e-05,7.999996e-05,7.999999e-05,8.4e-05,8.799999e-05,9.2e-05,9.599999e-05,1e-04,0.00011999998,0.00016000001,0.00019999997,0.00024,0.00028,0.00031999996,0.00036,0.0004,0.00043999997,0.00048,0.00052,0.00055999996,0.00059999997,0.00064000004,0.00068,0.00072,0.00076,0.0007999996,0.00080000004,0.00084,0.00088,0.0009200001,0.00096000003,0.001,0.0011999998,0.0016,0.0019999996,0.0023999996,0.0028,0.0031999995,0.0035999997,0.0039999997,0.004,0.0043999995,0.0047999998,0.0052,0.0055999993,0.0059999996,0.0064,0.006799999,0.0072,0.0075999997,0.007999997,0.0079999985,0.0084,0.0088,0.009199999,0.0095999995,0.01,0.011999999,0.016,0.019999998,0.023999998,0.028,0.031999998,0.036,0.04,0.040000003,0.043999996,0.047999997,0.052,0.055999998,0.06,0.064,0.067999996,0.072,0.076000005,0.07999996,0.08,0.084,0.088,0.092,0.096,0.1,0.11999998,0.16,0.19999996,0.23999998,0.28,0.31999996,0.35999998,0.4,0.43999997,0.48,0.52,0.55999994,0.59999996,0.64,0.67999995,0.71999997,0.76,0.79999995,0.84,0.88,0.92,0.96,1.0]\n",
    "minimum_loss = [12.199068069458008,10.287569999694824,8.428057670593262,6.696137428283691,5.538702964782715,4.687373161315918,4.059501647949219,3.656656265258789,3.457155704498291,3.325516700744629,3.174725294113159,2.993105411529541,2.772662401199341,2.507611036300659,2.195760726928711,1.8345900774002075,1.426107406616211,0.9818812012672424,0.5550472140312195,0.2830400764942169,0.2830425798892975,0.21830421686172485,0.17070499062538147,0.13883444666862488,0.12758783996105194,0.12315638363361359,0.10009343922138214,0.022998584434390068,0.006351709831506014,0.005078324116766453,0.004771950654685497,0.004673386458307505,0.004661232698708773,0.004636757075786591,0.004623145796358585,0.004604278597980738,0.0045907096937298775,0.004563584458082914,0.004564367700368166,0.004526973236352205,0.004533765837550163,0.004557443782687187,0.004603053443133831,0.00447818823158741,0.004484650678932667,0.004502951167523861,0.004616164602339268,0.00500076450407505,0.0056283138692379,0.0048086014576256275,0.00437261164188385,0.00505151366814971,0.0052724299021065235,0.005549023859202862,0.005764973349869251,0.005953838117420673,0.006143741775304079,0.006300668697804213,0.0064275446347892284,0.0065496391616761684,0.006680157035589218,0.006793366279453039,0.006896598730236292,0.0069457595236599445,0.007037411909550428,0.007094191387295723,0.00715602608397603,0.007218529935926199,0.007215419318526983,0.007279032841324806,0.00733697647228837,0.007450229488313198,0.007429111283272505,0.007506238296627998,0.007746902294456959,0.008289686404168606,0.008984302170574665,0.009617257863283157,0.01011795923113823,0.0107788797467947,0.011304347775876522,0.0116304662078619,0.011619005352258682,0.011790497228503227,0.011702973395586014,0.011884546838700771,0.012837653048336506,0.012833688408136368,0.013390054926276207,0.013657047413289547,0.013788939453661442,0.013915683142840862,0.014332626946270466,0.014518260024487972,0.014350385405123234,0.014661336317658424,0.015009942464530468,0.015280765481293201,0.015574362128973007,0.015930278226733208,0.018411044031381607,0.02647589147090912,0.026135995984077454,0.02557310089468956,0.01927211880683899,0.025239434093236923,0.042144015431404114,0.03771758824586868,0.03965884819626808,0.02181864343583584,0.04423406347632408,0.05317981541156769,0.042907778173685074,0.05452097952365875,0.04155665636062622,0.04972462356090546,0.04367397353053093,0.07090074568986893,0.042590875178575516,0.08229882270097733,0.06757473945617676,0.09713171422481537,0.07808905094861984,0.08564911037683487,0.11132657527923584,0.11153824627399445,0.14020493626594543,0.22732415795326233,0.11888231337070465,0.12719812989234924,0.33327990770339966,0.18740606307983398,0.36471953988075256,0.7714767456054688,0.6909691095352173,0.7433273196220398,0.439883828163147,0.9097825288772583,0.6054460406303406,0.6783227920532227,0.6899469494819641,2.1728146076202393,1.0038481950759888,1.763105869293213,2.2033491134643555,2.211721897125244,3.3589024543762207]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "__=ax.plot(np.log10(learning_rate),np.log10(minimum_loss),color='blue')\n",
    "__=ax.scatter(np.log10(learning_rate[min_index]),np.log10(minimum_loss[min_index]),color='red')\n",
    "legend = ax.legend([r'${\\rm learning \\ rate}$',r'${\\rm best \\ learning }$'])\n",
    "xlabel = ax.set_xlabel(r'$\\log_{10}{\\rm (learning \\ rate)}$')\n",
    "ylabel = ax.set_ylabel(r'$\\log_{10}{\\rm (loss\\_function)}$')\n",
    "#title = ax.set_title(r'${\\rm Learning \\ rate \\ optimization}$')\n",
    "plt.show()\n",
    "print(\"best learning rate =\",learning_rate[min_index],\"\\nloss =\",minimum_loss[min_index])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5030792b3492f6b12d94f1f48beca3d8e59ec05fd59d0aaaa48e684281ed297"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
