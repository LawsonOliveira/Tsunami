epoch: 1
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
epoch: 1 train step: 0 loss: 0.013709717
epoch: 1 train step: 50 loss: 0.013567624
epoch: 1 train step: 100 loss: 0.013293684
epoch: 1 train step: 150 loss: 0.013742811
epoch: 1 train step: 200 loss: 0.013369147
epoch: 1 train step: 250 loss: 0.013406358
epoch: 1 train step: 300 loss: 0.013106182
epoch: 1 train step: 350 loss: 0.013958963
epoch: 1 train step: 400 loss: 0.013370032
epoch: 1 train step: 450 loss: 0.012650946
epoch: 1 train step: 500 loss: 0.012997219
epoch: 1 train step: 550 loss: 0.012722982
epoch: 1 train step: 600 loss: 0.013165087
epoch: 1 train step: 650 loss: 0.013009869
epoch: 1 train step: 700 loss: 0.012482207
epoch: 1 train step: 750 loss: 0.012690263
epoch: 1 train step: 800 loss: 0.0127671715
epoch: 1 train step: 850 loss: 0.011807889
epoch: 1 train step: 900 loss: 0.012426514
epoch: 1 train step: 950 loss: 0.012374993
epoch: 1 train step: 1000 loss: 0.012195043
epoch: 1 train step: 1050 loss: 0.0119841
epoch: 1 train step: 1100 loss: 0.012000548
epoch: 1 train step: 1150 loss: 0.011806736
epoch: 1 train step: 1200 loss: 0.012258514
epoch: 1 train step: 1250 loss: 0.011816193
epoch: 1 train step: 1300 loss: 0.011811126
epoch: 1 train step: 1350 loss: 0.011969101
epoch: 1 train step: 1400 loss: 0.011939895
epoch: 1 train step: 1450 loss: 0.011365037
epoch: 1 train step: 1500 loss: 0.012422552
epoch: 1 train step: 1550 loss: 0.011763372
epoch: 1 train step: 1600 loss: 0.011889717
epoch: 1 train step: 1650 loss: 0.011399291
epoch: 1 train step: 1700 loss: 0.011846928
epoch: 1 train step: 1750 loss: 0.011852419
epoch: 1 train step: 1800 loss: 0.011567117
epoch: 1 train step: 1850 loss: 0.011628956
epoch: 1 train step: 1900 loss: 0.011592979
epoch: 1 train step: 1950 loss: 0.0116347335
epoch: 1 train step: 2000 loss: 0.01203562
epoch: 1 train step: 2050 loss: 0.011433012
epoch: 1 train step: 2100 loss: 0.012166181
epoch: 1 train step: 2150 loss: 0.011277853
epoch: 1 train step: 2200 loss: 0.011981376
epoch: 1 train step: 2250 loss: 0.012126306
epoch: 1 train step: 2300 loss: 0.01183773
epoch: 1 train step: 2350 loss: 0.011838174
epoch: 1 train step: 2400 loss: 0.012095548
epoch: 1 train step: 2450 loss: 0.012008171
epoch: 1 train step: 2500 loss: 0.012065523
epoch: 1 train step: 2550 loss: 0.011909677
epoch: 1 train step: 2600 loss: 0.012204895
epoch: 1 train step: 2650 loss: 0.01175205
epoch: 1 train step: 2700 loss: 0.011665653
epoch: 1 train step: 2750 loss: 0.0117139695
epoch: 1 train step: 2800 loss: 0.0115524
epoch: 1 train step: 2850 loss: 0.01163395
epoch: 1 train step: 2900 loss: 0.011483455
epoch: 1 train step: 2950 loss: 0.0115125105
epoch: 1 train step: 3000 loss: 0.011349287
epoch: 1 train step: 3050 loss: 0.01137452
epoch: 1 train step: 3100 loss: 0.011218724
epoch: 1 train step: 3150 loss: 0.011724085
epoch: 1 train step: 3200 loss: 0.011883966
epoch: 1 train step: 3250 loss: 0.01182598
epoch: 1 train step: 3300 loss: 0.011529053
epoch: 1 train step: 3350 loss: 0.0114448
epoch: 1 train step: 3400 loss: 0.011790679
epoch: 1 train step: 3450 loss: 0.011712398
epoch: 1 train step: 3500 loss: 0.012055508
epoch: 1 train step: 3550 loss: 0.011786312
epoch: 1 train step: 3600 loss: 0.012057896
epoch: 1 train step: 3650 loss: 0.011486857
epoch: 1 train step: 3700 loss: 0.01169371
epoch: 1 train step: 3750 loss: 0.0120138535
epoch: 1 train step: 3800 loss: 0.01158889
epoch: 1 train step: 3850 loss: 0.011729484
epoch: 1 train step: 3900 loss: 0.011589086
epoch: 1 train step: 3950 loss: 0.011909043
epoch: 1 train step: 4000 loss: 0.012274483
epoch: 1 train step: 4050 loss: 0.011497623
epoch: 1 train step: 4100 loss: 0.011713465
epoch: 1 train step: 4150 loss: 0.012137196
epoch: 1 train step: 4200 loss: 0.011317373
epoch: 1 train step: 4250 loss: 0.011197874
epoch: 1 train step: 4300 loss: 0.011602484
epoch: 1 train step: 4350 loss: 0.011524379
epoch: 1 train step: 4400 loss: 0.011638881
epoch: 1 train step: 4450 loss: 0.011562873
epoch: 1 train step: 4500 loss: 0.011904991
epoch: 1 train step: 4550 loss: 0.011131135
epoch: 1 train step: 4600 loss: 0.011581506
epoch: 1 train step: 4650 loss: 0.011534238
epoch: 1 train step: 4700 loss: 0.011099917
epoch: 1 train step: 4750 loss: 0.011571551
epoch: 1 train step: 4800 loss: 0.01136859
epoch: 1 train step: 4850 loss: 0.011571059
epoch: 1 train step: 4900 loss: 0.01148217
epoch: 1 train step: 4950 loss: 0.0114043765
mean_loss: 0.012007065 val_mae: 0.4059897 duration epoch: 155.07529377937317