epoch: 1
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
epoch: 1 train step: 0 loss: 0.014239718
epoch: 1 train step: 50 loss: 0.014205622
epoch: 1 train step: 100 loss: 0.013866489
epoch: 1 train step: 150 loss: 0.014138203
epoch: 1 train step: 200 loss: 0.013630934
epoch: 1 train step: 250 loss: 0.013384663
epoch: 1 train step: 300 loss: 0.013379517
epoch: 1 train step: 350 loss: 0.013127464
epoch: 1 train step: 400 loss: 0.013340605
epoch: 1 train step: 450 loss: 0.013517089
epoch: 1 train step: 500 loss: 0.012878148
epoch: 1 train step: 550 loss: 0.012904864
epoch: 1 train step: 600 loss: 0.012876043
epoch: 1 train step: 650 loss: 0.012948319
epoch: 1 train step: 700 loss: 0.012779695
epoch: 1 train step: 750 loss: 0.012563739
epoch: 1 train step: 800 loss: 0.012629494
epoch: 1 train step: 850 loss: 0.012253805
epoch: 1 train step: 900 loss: 0.013254166
epoch: 1 train step: 950 loss: 0.012593796
epoch: 1 train step: 1000 loss: 0.012482405
epoch: 1 train step: 1050 loss: 0.0122427195
epoch: 1 train step: 1100 loss: 0.012195272
epoch: 1 train step: 1150 loss: 0.012613556
epoch: 1 train step: 1200 loss: 0.012087293
epoch: 1 train step: 1250 loss: 0.011919836
epoch: 1 train step: 1300 loss: 0.012259494
epoch: 1 train step: 1350 loss: 0.0124324355
epoch: 1 train step: 1400 loss: 0.011873221
epoch: 1 train step: 1450 loss: 0.012249463
epoch: 1 train step: 1500 loss: 0.011525908
epoch: 1 train step: 1550 loss: 0.011932914
epoch: 1 train step: 1600 loss: 0.011612367
epoch: 1 train step: 1650 loss: 0.011923162
epoch: 1 train step: 1700 loss: 0.011156762
epoch: 1 train step: 1750 loss: 0.011522057
epoch: 1 train step: 1800 loss: 0.011319205
epoch: 1 train step: 1850 loss: 0.011697909
epoch: 1 train step: 1900 loss: 0.012066452
epoch: 1 train step: 1950 loss: 0.011739332
epoch: 1 train step: 2000 loss: 0.011784427
epoch: 1 train step: 2050 loss: 0.01183873
epoch: 1 train step: 2100 loss: 0.011983557
epoch: 1 train step: 2150 loss: 0.011309962
epoch: 1 train step: 2200 loss: 0.01181593
epoch: 1 train step: 2250 loss: 0.011891297
epoch: 1 train step: 2300 loss: 0.012308069
epoch: 1 train step: 2350 loss: 0.012065854
epoch: 1 train step: 2400 loss: 0.0117777325
epoch: 1 train step: 2450 loss: 0.012305418
epoch: 1 train step: 2500 loss: 0.011680551
epoch: 1 train step: 2550 loss: 0.011733465
epoch: 1 train step: 2600 loss: 0.011715731
epoch: 1 train step: 2650 loss: 0.012204229
epoch: 1 train step: 2700 loss: 0.0116530685
epoch: 1 train step: 2750 loss: 0.011688306
epoch: 1 train step: 2800 loss: 0.011649502
epoch: 1 train step: 2850 loss: 0.011547897
epoch: 1 train step: 2900 loss: 0.011684064
epoch: 1 train step: 2950 loss: 0.012097497
epoch: 1 train step: 3000 loss: 0.011246841
epoch: 1 train step: 3050 loss: 0.0114817675
epoch: 1 train step: 3100 loss: 0.011541398
epoch: 1 train step: 3150 loss: 0.011340991
epoch: 1 train step: 3200 loss: 0.011438059
epoch: 1 train step: 3250 loss: 0.011813872
epoch: 1 train step: 3300 loss: 0.011598901
epoch: 1 train step: 3350 loss: 0.011781861
epoch: 1 train step: 3400 loss: 0.011800898
epoch: 1 train step: 3450 loss: 0.011701981
epoch: 1 train step: 3500 loss: 0.011813583
epoch: 1 train step: 3550 loss: 0.01165238
epoch: 1 train step: 3600 loss: 0.011880498
epoch: 1 train step: 3650 loss: 0.011652439
epoch: 1 train step: 3700 loss: 0.011636715
epoch: 1 train step: 3750 loss: 0.011825212
epoch: 1 train step: 3800 loss: 0.012054017
epoch: 1 train step: 3850 loss: 0.011325831
epoch: 1 train step: 3900 loss: 0.011462867
epoch: 1 train step: 3950 loss: 0.011621486
epoch: 1 train step: 4000 loss: 0.011547836
epoch: 1 train step: 4050 loss: 0.012389466
epoch: 1 train step: 4100 loss: 0.011684673
epoch: 1 train step: 4150 loss: 0.011554951
epoch: 1 train step: 4200 loss: 0.012128364
epoch: 1 train step: 4250 loss: 0.011090016
epoch: 1 train step: 4300 loss: 0.011688501
epoch: 1 train step: 4350 loss: 0.01156952
epoch: 1 train step: 4400 loss: 0.011778746
epoch: 1 train step: 4450 loss: 0.011612387
epoch: 1 train step: 4500 loss: 0.011367162
epoch: 1 train step: 4550 loss: 0.01122506
epoch: 1 train step: 4600 loss: 0.011585664
epoch: 1 train step: 4650 loss: 0.010847525
epoch: 1 train step: 4700 loss: 0.011171082
epoch: 1 train step: 4750 loss: 0.011771723
epoch: 1 train step: 4800 loss: 0.011242446
epoch: 1 train step: 4850 loss: 0.011410866
epoch: 1 train step: 4900 loss: 0.0115029905
epoch: 1 train step: 4950 loss: 0.011200495
mean_loss: 0.012025712 val_mae: 0.40554023 duration epoch: 141.65168046951294