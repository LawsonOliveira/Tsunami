epoch: 1
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
epoch: 1 train step: 0 loss: 0.0146126
epoch: 1 train step: 50 loss: 0.014607463
epoch: 1 train step: 100 loss: 0.013570366
epoch: 1 train step: 150 loss: 0.013510869
epoch: 1 train step: 200 loss: 0.013784704
epoch: 1 train step: 250 loss: 0.013477642
epoch: 1 train step: 300 loss: 0.01354651
epoch: 1 train step: 350 loss: 0.013462689
epoch: 1 train step: 400 loss: 0.013355775
epoch: 1 train step: 450 loss: 0.013138847
epoch: 1 train step: 500 loss: 0.013193867
epoch: 1 train step: 550 loss: 0.012865458
epoch: 1 train step: 600 loss: 0.012894319
epoch: 1 train step: 650 loss: 0.013068059
epoch: 1 train step: 700 loss: 0.0135138985
epoch: 1 train step: 750 loss: 0.012174631
epoch: 1 train step: 800 loss: 0.01286488
epoch: 1 train step: 850 loss: 0.012721331
epoch: 1 train step: 900 loss: 0.012445474
epoch: 1 train step: 950 loss: 0.01222272
epoch: 1 train step: 1000 loss: 0.012622865
epoch: 1 train step: 1050 loss: 0.012144504
epoch: 1 train step: 1100 loss: 0.012188075
epoch: 1 train step: 1150 loss: 0.012040648
epoch: 1 train step: 1200 loss: 0.012444522
epoch: 1 train step: 1250 loss: 0.01255241
epoch: 1 train step: 1300 loss: 0.012213534
epoch: 1 train step: 1350 loss: 0.012057485
epoch: 1 train step: 1400 loss: 0.012100948
epoch: 1 train step: 1450 loss: 0.012001526
epoch: 1 train step: 1500 loss: 0.0121392
epoch: 1 train step: 1550 loss: 0.011762997
epoch: 1 train step: 1600 loss: 0.011777576
epoch: 1 train step: 1650 loss: 0.011775221
epoch: 1 train step: 1700 loss: 0.011608686
epoch: 1 train step: 1750 loss: 0.011550585
epoch: 1 train step: 1800 loss: 0.01159904
epoch: 1 train step: 1850 loss: 0.011731872
epoch: 1 train step: 1900 loss: 0.0117513705
epoch: 1 train step: 1950 loss: 0.011976508
epoch: 1 train step: 2000 loss: 0.011615269
epoch: 1 train step: 2050 loss: 0.012211035
epoch: 1 train step: 2100 loss: 0.01226771
epoch: 1 train step: 2150 loss: 0.0123055475
epoch: 1 train step: 2200 loss: 0.012232758
epoch: 1 train step: 2250 loss: 0.011576438
epoch: 1 train step: 2300 loss: 0.011747168
epoch: 1 train step: 2350 loss: 0.012138088
epoch: 1 train step: 2400 loss: 0.011724068
epoch: 1 train step: 2450 loss: 0.012179153
epoch: 1 train step: 2500 loss: 0.0117959725
epoch: 1 train step: 2550 loss: 0.011956178
epoch: 1 train step: 2600 loss: 0.01184053
epoch: 1 train step: 2650 loss: 0.011707117
epoch: 1 train step: 2700 loss: 0.011544196
epoch: 1 train step: 2750 loss: 0.012106406
epoch: 1 train step: 2800 loss: 0.011635532
epoch: 1 train step: 2850 loss: 0.011592174
epoch: 1 train step: 2900 loss: 0.01179269
epoch: 1 train step: 2950 loss: 0.011886153
epoch: 1 train step: 3000 loss: 0.011798641
epoch: 1 train step: 3050 loss: 0.011509312
epoch: 1 train step: 3100 loss: 0.011862144
epoch: 1 train step: 3150 loss: 0.01250151
epoch: 1 train step: 3200 loss: 0.011755102
epoch: 1 train step: 3250 loss: 0.011384558
epoch: 1 train step: 3300 loss: 0.011506424
epoch: 1 train step: 3350 loss: 0.011437203
epoch: 1 train step: 3400 loss: 0.011494366
epoch: 1 train step: 3450 loss: 0.011754876
epoch: 1 train step: 3500 loss: 0.011724231
epoch: 1 train step: 3550 loss: 0.012115267
epoch: 1 train step: 3600 loss: 0.012100516
epoch: 1 train step: 3650 loss: 0.012223028
epoch: 1 train step: 3700 loss: 0.011899246
epoch: 1 train step: 3750 loss: 0.011249727
epoch: 1 train step: 3800 loss: 0.012000728
epoch: 1 train step: 3850 loss: 0.011705108
epoch: 1 train step: 3900 loss: 0.011515108
epoch: 1 train step: 3950 loss: 0.012058603
epoch: 1 train step: 4000 loss: 0.011795858
epoch: 1 train step: 4050 loss: 0.011835318
epoch: 1 train step: 4100 loss: 0.01118833
epoch: 1 train step: 4150 loss: 0.01129131
epoch: 1 train step: 4200 loss: 0.011739358
epoch: 1 train step: 4250 loss: 0.011546856
epoch: 1 train step: 4300 loss: 0.011728684
epoch: 1 train step: 4350 loss: 0.011271154
epoch: 1 train step: 4400 loss: 0.011214098
epoch: 1 train step: 4450 loss: 0.011645906
epoch: 1 train step: 4500 loss: 0.011579404
epoch: 1 train step: 4550 loss: 0.011613753
epoch: 1 train step: 4600 loss: 0.011433518
epoch: 1 train step: 4650 loss: 0.011021692
epoch: 1 train step: 4700 loss: 0.011285191
epoch: 1 train step: 4750 loss: 0.01128301
epoch: 1 train step: 4800 loss: 0.011552023
epoch: 1 train step: 4850 loss: 0.011010924
epoch: 1 train step: 4900 loss: 0.011454966
epoch: 1 train step: 4950 loss: 0.011460533
mean_loss: 0.012017978 val_mae: 0.40929002 duration epoch: 154.23348903656006