epoch: 1
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
epoch: 1 train step: 0 loss: 0.013881284
epoch: 1 train step: 50 loss: 0.013625179
epoch: 1 train step: 100 loss: 0.01382118
epoch: 1 train step: 150 loss: 0.014082724
epoch: 1 train step: 200 loss: 0.013725171
epoch: 1 train step: 250 loss: 0.013339996
epoch: 1 train step: 300 loss: 0.013685206
epoch: 1 train step: 350 loss: 0.013471475
epoch: 1 train step: 400 loss: 0.013109328
epoch: 1 train step: 450 loss: 0.013512649
epoch: 1 train step: 500 loss: 0.013153684
epoch: 1 train step: 550 loss: 0.013155128
epoch: 1 train step: 600 loss: 0.012988657
epoch: 1 train step: 650 loss: 0.012785166
epoch: 1 train step: 700 loss: 0.012555378
epoch: 1 train step: 750 loss: 0.0124547
epoch: 1 train step: 800 loss: 0.012230504
epoch: 1 train step: 850 loss: 0.012616629
epoch: 1 train step: 900 loss: 0.012611968
epoch: 1 train step: 950 loss: 0.012436586
epoch: 1 train step: 1000 loss: 0.0121214315
epoch: 1 train step: 1050 loss: 0.012200458
epoch: 1 train step: 1100 loss: 0.012440713
epoch: 1 train step: 1150 loss: 0.011715056
epoch: 1 train step: 1200 loss: 0.012234685
epoch: 1 train step: 1250 loss: 0.012021054
epoch: 1 train step: 1300 loss: 0.011986005
epoch: 1 train step: 1350 loss: 0.011640071
epoch: 1 train step: 1400 loss: 0.012151463
epoch: 1 train step: 1450 loss: 0.011539223
epoch: 1 train step: 1500 loss: 0.011857158
epoch: 1 train step: 1550 loss: 0.011508431
epoch: 1 train step: 1600 loss: 0.011367371
epoch: 1 train step: 1650 loss: 0.011230679
epoch: 1 train step: 1700 loss: 0.01206686
epoch: 1 train step: 1750 loss: 0.011587522
epoch: 1 train step: 1800 loss: 0.012090735
epoch: 1 train step: 1850 loss: 0.011563558
epoch: 1 train step: 1900 loss: 0.0114843035
epoch: 1 train step: 1950 loss: 0.01137585
epoch: 1 train step: 2000 loss: 0.012094799
epoch: 1 train step: 2050 loss: 0.011989161
epoch: 1 train step: 2100 loss: 0.011968308
epoch: 1 train step: 2150 loss: 0.0120586045
epoch: 1 train step: 2200 loss: 0.011572024
epoch: 1 train step: 2250 loss: 0.011910898
epoch: 1 train step: 2300 loss: 0.011834486
epoch: 1 train step: 2350 loss: 0.012671572
epoch: 1 train step: 2400 loss: 0.012060155
epoch: 1 train step: 2450 loss: 0.012031257
epoch: 1 train step: 2500 loss: 0.011991555
epoch: 1 train step: 2550 loss: 0.011793548
epoch: 1 train step: 2600 loss: 0.012221385
epoch: 1 train step: 2650 loss: 0.01161002
epoch: 1 train step: 2700 loss: 0.012003443
epoch: 1 train step: 2750 loss: 0.01181815
epoch: 1 train step: 2800 loss: 0.011765314
epoch: 1 train step: 2850 loss: 0.011416694
epoch: 1 train step: 2900 loss: 0.011529073
epoch: 1 train step: 2950 loss: 0.012020512
epoch: 1 train step: 3000 loss: 0.011843404
epoch: 1 train step: 3050 loss: 0.011918332
epoch: 1 train step: 3100 loss: 0.011861581
epoch: 1 train step: 3150 loss: 0.0115633225
epoch: 1 train step: 3200 loss: 0.011817586
epoch: 1 train step: 3250 loss: 0.011759587
epoch: 1 train step: 3300 loss: 0.01162031
epoch: 1 train step: 3350 loss: 0.011773473
epoch: 1 train step: 3400 loss: 0.011728853
epoch: 1 train step: 3450 loss: 0.011283208
epoch: 1 train step: 3500 loss: 0.01180361
epoch: 1 train step: 3550 loss: 0.011448629
epoch: 1 train step: 3600 loss: 0.011862064
epoch: 1 train step: 3650 loss: 0.011889444
epoch: 1 train step: 3700 loss: 0.0112546105
epoch: 1 train step: 3750 loss: 0.011903534
epoch: 1 train step: 3800 loss: 0.0119006755
epoch: 1 train step: 3850 loss: 0.011707433
epoch: 1 train step: 3900 loss: 0.01149022
epoch: 1 train step: 3950 loss: 0.011448406
epoch: 1 train step: 4000 loss: 0.011189757
epoch: 1 train step: 4050 loss: 0.011394821
epoch: 1 train step: 4100 loss: 0.01153357
epoch: 1 train step: 4150 loss: 0.011131646
epoch: 1 train step: 4200 loss: 0.011664565
epoch: 1 train step: 4250 loss: 0.011423549
epoch: 1 train step: 4300 loss: 0.011077844
epoch: 1 train step: 4350 loss: 0.011657176
epoch: 1 train step: 4400 loss: 0.011491599
epoch: 1 train step: 4450 loss: 0.011396142
epoch: 1 train step: 4500 loss: 0.0112487385
epoch: 1 train step: 4550 loss: 0.011499882
epoch: 1 train step: 4600 loss: 0.011848361
epoch: 1 train step: 4650 loss: 0.011351212
epoch: 1 train step: 4700 loss: 0.011468847
epoch: 1 train step: 4750 loss: 0.011621423
epoch: 1 train step: 4800 loss: 0.010968671
epoch: 1 train step: 4850 loss: 0.011562868
epoch: 1 train step: 4900 loss: 0.011643646
epoch: 1 train step: 4950 loss: 0.011478511
mean_loss: 0.01201694 val_mae: 0.40479857 duration epoch: 140.91512989997864