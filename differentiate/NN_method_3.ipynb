{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3961130884003023103\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5730467840\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4587379600596016239\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "cwd: C:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\n",
      "changed to: C:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from matplotlib import cm\n",
    "from sympy import Matrix\n",
    "import sympy as sm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "################### check tensorflow is using GPU ###################\n",
    "from tensorflow.python.client import device_lib\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "print(device_lib.list_local_devices())\n",
    "# what if empty...\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# On windows systems you cannont install NCCL that is required for multi GPU\n",
    "# So we need to follow hierarchical copy method or reduce to single GPU (less efficient than the former)\n",
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    devices=['GPU:0'], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "DTYPE = 'float32'\n",
    "\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "\n",
    "################### proper to the computer used ###################\n",
    "__file__ = 'C:/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami'\n",
    "\n",
    "print('\\ncwd:', os.getcwd())\n",
    "os.chdir(__file__)\n",
    "print('changed to:', os.getcwd(), '\\n')\n",
    "\n",
    "\n",
    "################### Create the model ###################\n",
    "\n",
    "def generate_model(l_units, noise=False):\n",
    "    # méthode API Sequential\n",
    "    n_hidden = len(l_units)\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape=(2))\n",
    "    ])\n",
    "    if noise:\n",
    "        model.add(keras.layers.GaussianNoise(stddev=1e-4))\n",
    "    for i in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(\n",
    "            l_units[i], activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.Dense(1, use_bias=False)) \n",
    "    # use_bias=False otherwise returns error after\n",
    "    # May be the cause of a stagnation in the validation ? (can circumvent by creating a layer class only adding a bias)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_0():\n",
    "    generate_model()\n",
    "\n",
    "################### define the EDP to solve ###################\n",
    "# Given EDO\n",
    "def f(X):\n",
    "    return tf.sin(np.pi*X[:, 0])*tf.sin(np.pi*X[:, 1])\n",
    "\n",
    "\n",
    "def boundary_conditions(X):\n",
    "    return 0\n",
    "\n",
    "\n",
    "def residual(du_dxx, du_dyy, f_ind):\n",
    "    return du_dxx+du_dyy+f_ind\n",
    "\n",
    "\n",
    "def differentiate(model, x):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        x1, x2 = x[:, 0:1], x[:, 1:2]\n",
    "        tape.watch(x1)\n",
    "        tape.watch(x2)\n",
    "        u = model(tf.stack([x1[:, 0], x2[:, 0]], axis=1))\n",
    "        du_dx = tape.gradient(u, x1)\n",
    "        du_dy = tape.gradient(u, x2)\n",
    "    du_dxx = tape.gradient(du_dx, x1)\n",
    "    du_dyy = tape.gradient(du_dy, x2)\n",
    "    return du_dx, du_dxx, du_dy, du_dyy\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "################### Method 3: g automatically respects the boundary conditions ########################\n",
    "# article : 1997_Artificial_neural_networks_for_solving_ordinary_and_partial_differential_equations.pdf\n",
    "\n",
    "################### Set F here ###################\n",
    "# Dummy F\n",
    "\n",
    "\n",
    "x, y = sm.symbols('x,y')\n",
    "\n",
    "\n",
    "def expr_dummy_F():\n",
    "    return x*(1-x)*y*(1-y)\n",
    "\n",
    "\n",
    "expr_F = expr_dummy_F()\n",
    "dexpr_F_dx = sm.diff(expr_F, x, 1)\n",
    "dexpr_F_dxx = sm.diff(dexpr_F_dx, x, 1)\n",
    "dexpr_F_dy = sm.diff(expr_F, y, 1)\n",
    "dexpr_F_dyy = sm.diff(dexpr_F_dy, y, 1)\n",
    "\n",
    "\n",
    "# remark: You can forget a no lambdified expression => here we greatly avoid 'for' loops\n",
    "\n",
    "expr_F = sm.lambdify([x, y], Matrix([expr_F]), 'numpy')\n",
    "dexpr_F_dx = sm.lambdify([x, y], Matrix([dexpr_F_dx]), 'numpy')\n",
    "dexpr_F_dxx = sm.lambdify([x, y], Matrix([dexpr_F_dxx]), 'numpy')\n",
    "dexpr_F_dy = sm.lambdify([x, y], Matrix([dexpr_F_dy]), 'numpy')\n",
    "dexpr_F_dyy = sm.lambdify([x, y], Matrix([dexpr_F_dyy]), 'numpy')\n",
    "\n",
    "\n",
    "def evaluate_F_and_diff(X):\n",
    "    F = tf.squeeze(tf.transpose(expr_F(X[:, 0], X[:, 1])), axis=-1)\n",
    "    dF_dx = tf.expand_dims(dexpr_F_dx(X[:, 0], X[:, 1]), axis=-1)\n",
    "    dF_dxx = tf.expand_dims(dexpr_F_dxx(X[:, 0], X[:, 1]), axis=-1)\n",
    "    dF_dy = tf.expand_dims(dexpr_F_dy(X[:, 0], X[:, 1]), axis=-1)\n",
    "    dF_dyy = tf.expand_dims(dexpr_F_dyy(X[:, 0], X[:, 1]), axis=-1)\n",
    "\n",
    "    return F, dF_dx, dF_dxx, dF_dy, dF_dyy\n",
    "\n",
    " # oddly enough expr_F and dexpr_F_d... do not have the same output\n",
    "\n",
    "\n",
    "# # #### F of F_functions\n",
    "\n",
    "\n",
    "# frontier_coords = Pstud._set_coords_rectangle(1, 1, 10)\n",
    "\n",
    "# l_orders = [(1, 0), (2, 0), (0, 1), (0, 2)]\n",
    "# strfn = 'sinxpy_real'\n",
    "# F = F2D(frontier_coords, strfn, l_orders=l_orders)\n",
    "\n",
    "# # prepare to infer on large matrices :\n",
    "# F.expr = sm.lambdify(F.variables, Matrix([F.expr]), 'numpy')\n",
    "# for t_order in l_orders:\n",
    "#     F.reduced_tab_diff[F.dico_order_to_index[t_order]] = sm.lambdify(\n",
    "#         F.variables, F.reduced_tab_diff[F.dico_order_to_index[t_order]], 'numpy')\n",
    "\n",
    "\n",
    "# def evaluate_F_and_diff(X):\n",
    "#     '''\n",
    "#     evaluate F and its differentiates get in F.reduced_tab_diff\n",
    "#     Variables:\n",
    "#     -X: an array or tensor tf of the coordinates\n",
    "\n",
    "#     Returns:\n",
    "#     -l_eval: list of the evaluations. To know which element corresponds to which order, use F.dico_order_to_index and increment the values of 1.\n",
    "\n",
    "#     remark: to add to F2D class\n",
    "#     '''\n",
    "#     l_eval = [tf.squeeze(tf.transpose(F.expr(X[:, 0], X[:, 1])), axis=-1)]\n",
    "\n",
    "#     for i, t_order in enumerate(F.reduced_tab_diff):\n",
    "#         l_eval.append(tf.expand_dims(\n",
    "#             F.reduced_tab_diff[i](X[:, 0], X[:, 1]), axis=-1))\n",
    "\n",
    "#     return l_eval\n",
    "\n",
    "\n",
    "\n",
    "################### Set A here ###################\n",
    "\n",
    "A = 0\n",
    "dA_dxx = 0\n",
    "dA_dyy = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_config(config,id_add):\n",
    "    '''\n",
    "    A run of the full algorithm described in the paper of 1997. \n",
    "    Variables:\n",
    "    -config (dict): The hyperparameters used to construct the model and set the training loop are in config.\n",
    "    -id_add (int): add id_add to the trial_id to avoid overwriting previous trials \n",
    "    '''\n",
    "    print('config:\\n',config)\n",
    "    config_model = config['config_model']\n",
    "    config_training = config['config_training']\n",
    "\n",
    "    l_units = config_model['l_units']\n",
    "    noise = config_model['noise']\n",
    "    learning_rate = config_model['learning_rate']\n",
    "    if config_model['optimizer'] == \"Adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        print('optimizer is not known !')\n",
    "\n",
    "    learning_rate = config_model['learning_rate']\n",
    "\n",
    "\n",
    "    # generate model\n",
    "    model = generate_model(l_units, noise=noise)\n",
    "\n",
    "\n",
    "    # Universal Approximator\n",
    "    # @tf.function  # TODO: learn to use it to accelerate the computations\n",
    "    def g_3(X, training=True):\n",
    "        # F_x = Pstud._eval_polynome_numpy(F_xpy_real,x[0,0],x[0,1])\n",
    "        N_X = model(X, training=training)\n",
    "        return tf.squeeze(tf.transpose(expr_F(X[:, 0], X[:, 1])), axis=-1)*N_X\n",
    "\n",
    "\n",
    "    # Custom loss function to approximate the derivatives\n",
    "\n",
    "    def custom_loss_3(tf_sample_coords):\n",
    "        dN_dx, dN_dxx, dN_dy, dN_dyy = differentiate(\n",
    "            model, tf_sample_coords)\n",
    "        f_r = tf.reshape(f(tf_sample_coords), [batch_size, 1])\n",
    "\n",
    "        F, dF_dx, dF_dxx, dF_dy, dF_dyy = evaluate_F_and_diff(tf_sample_coords)\n",
    "\n",
    "        dg_dxx = dF_dxx + 2*dF_dx*dN_dx + F*dN_dxx + dA_dxx\n",
    "        dg_dyy = dF_dyy + 2*dF_dy*dN_dy + F*dN_dyy + dA_dyy\n",
    "        res = residual(dg_dxx, dg_dyy, f_r)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(res))\n",
    "        return loss\n",
    "\n",
    "    # train of method 3:\n",
    "\n",
    "    def train_step_3(tf_sample_coords):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = custom_loss_3(tf_sample_coords)\n",
    "        trainable_variables = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    mae_metric = tf.keras.metrics.MeanAbsoluteError(\n",
    "        name=\"mean_absolute_error\", dtype=None)\n",
    "\n",
    "\n",
    "    def validate(validation_coords):\n",
    "        _, dg_dxx, _, dg_dyy = differentiate(g_3, validation_coords)\n",
    "        f_r = tf.reshape(f(validation_coords), [tf.shape(validation_coords)[0], 1])\n",
    "        res = residual(dg_dxx, dg_dyy, f_r)\n",
    "        val_mae = mae_metric(res, tf.zeros(tf.shape(res))).numpy()\n",
    "        return val_mae\n",
    "\n",
    "\n",
    "    # Training the Model of method 3:\n",
    "\n",
    "\n",
    "    trial_id = config['trial_id']+id_add\n",
    "    epochs_max = config_training['epochs_max']\n",
    "    n_trains = config_training['n_trains']\n",
    "    batch_size = config_training['batch_size']\n",
    "    display_step = config_training['display_step']\n",
    "    tol = config_training['tol']\n",
    "    patience = config_training['patience']\n",
    "\n",
    "\n",
    "\n",
    "    ## !!! to change according to the way the folders are arranged\n",
    "    folder_dir = f'differentiate/hypertuning/byHand/trial_{trial_id}/'\n",
    "\n",
    "\n",
    "\n",
    "    # TODO: learn how to use Yaml instead... (piece of advice from Jules S.)\n",
    "\n",
    "    if not(os.path.exists(folder_dir)):\n",
    "        os.mkdir(folder_dir)\n",
    "    with open(folder_dir+f'config_{trial_id}.json', 'w') as fp:\n",
    "        json.dump(config, fp, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "    history = {'mean_loss': [], 'val_mae': []}\n",
    "\n",
    "    epoch = 0\n",
    "    val_mae = np.infty\n",
    "    val_mae_reached = (val_mae <= tol)\n",
    "    EarlyStopped = False\n",
    "\n",
    "    # tf.keras.backend.set_learning_phase(1) # 'globally' activate training mode, slightly too strong maybe : check training mode for GaussianNoise layer\n",
    "    while not(EarlyStopped) and not(val_mae_reached) and epoch < epochs_max:\n",
    "        epoch += 1\n",
    "        time_start = time()\n",
    "        print('epoch:', epoch, end=' ')\n",
    "        losses = []\n",
    "\n",
    "        indices = np.random.randint(tf_coords.shape[0], size=batch_size)\n",
    "        tf_sample_coords = tf.convert_to_tensor([tf_coords[i] for i in indices])\n",
    "        for k in range(n_trains):\n",
    "            if k % display_step == display_step-1:\n",
    "                print('.', end='')\n",
    "            losses.append(train_step_3(tf_sample_coords))\n",
    "        loss = np.mean(losses)\n",
    "\n",
    "        # create validation_coords\n",
    "        indices = np.random.randint(tf_coords.shape[0], size=100)\n",
    "        tf_val_coords = tf.convert_to_tensor([tf_coords[i] for i in indices])\n",
    "        tf_val_coords = tf_val_coords + \\\n",
    "            tf.random.normal(shape=tf.shape(\n",
    "                tf_val_coords).numpy(), mean=0, stddev=1e-3)\n",
    "        val_mae = validate(tf_val_coords)\n",
    "\n",
    "        print(\"mean_loss:\", loss, end=' ')\n",
    "        print('val_mae:', val_mae, end=' ')\n",
    "        history['mean_loss'].append(loss)\n",
    "        history['val_mae'].append(val_mae)\n",
    "\n",
    "        # time_end_training = time()\n",
    "        # print('duration training :', time_end_training-time_start, end=' ')\n",
    "\n",
    "        val_mae_reached = (val_mae <= tol)\n",
    "\n",
    "        if val_mae_reached:\n",
    "            print(f'\\n tolerance set is reached : {val_mae}<={tol}')\n",
    "\n",
    "        model.save(\n",
    "            folder_dir+f'model_poisson_trial_{trial_id}_epoch_{epoch}_val_mae_{val_mae:6f}.h5')\n",
    "\n",
    "        if (len(history['val_mae']) >= patience+1) and np.argmin(history['val_mae'][-(patience+1):]) == 0:\n",
    "            print('\\n EarlyStopping activated', end = ' ')\n",
    "            EarlyStopped = True\n",
    "            \n",
    "\n",
    "        elif (len(history['val_mae']) >= patience+1):\n",
    "            # clean the savings folder\n",
    "            r_val_mae_epoch = epoch-patience\n",
    "            r_val_mae = history['val_mae'][-(patience+1)]\n",
    "            file_path = folder_dir + \\\n",
    "                f'model_poisson_trial_{trial_id}_epoch_{r_val_mae_epoch}_val_mae_{r_val_mae:6f}.h5'\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "            else:\n",
    "                print(file_path)\n",
    "                print(\"The system cannot find the file specified\")\n",
    "\n",
    "        time_end_epoch = time()\n",
    "        duration_epoch = time_end_epoch-time_start\n",
    "        print('duration epoch:', duration_epoch)\n",
    "        print()\n",
    "\n",
    "\n",
    "    # not optimized\n",
    "    min_val_mae = np.min(history['val_mae'])\n",
    "    min_val_mae_epoch = np.argmin(history['val_mae'])+1\n",
    "\n",
    "\n",
    "    model = keras.models.load_model(folder_dir+f'model_poisson_trial_{trial_id}_epoch_{min_val_mae_epoch}_val_mae_{min_val_mae:6f}.h5')\n",
    "    os.rename(folder_dir+f'model_poisson_trial_{trial_id}_epoch_{min_val_mae_epoch}_val_mae_{min_val_mae:6f}.h5', folder_dir+f'best_model_poisson_trial_{trial_id}_epoch_{min_val_mae_epoch}_val_mae_{min_val_mae:6f}.h5')\n",
    "    print('best model loaded and renamed')\n",
    "\n",
    "    # tf.keras.backend.set_learning_phase(0) # 'globally' disable training mode\n",
    "    print(\"val_mae>tol:\", val_mae > tol)\n",
    "\n",
    "    plt.plot(np.arange(0, epoch), history['mean_loss'], label='mean_loss')\n",
    "    plt.plot(np.arange(0, epoch), history['val_mae'], label='val_mae')\n",
    "    # plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title(\n",
    "        f'epochs_max = {epochs_max},n_trains={n_trains},batch_size={batch_size}')\n",
    "    plt.legend()\n",
    "    plt.savefig(folder_dir+f\"history_trial_{trial_id}.png\",transparent=False)\n",
    "    print(folder_dir+f\"history_trial_{trial_id}.png\")\n",
    "\n",
    "    plt.plot(list(range(0, len(history['val_mae']))), history['val_mae'])\n",
    "    plt.show()\n",
    "\n",
    "    print(np.min(history['val_mae']))\n",
    "\n",
    "\n",
    "# Does not learn with F = F2D(..., 'sinxpy_real') or 'xpy_real'\n",
    "# - take a look at dF_dx and so on\n",
    "# - ... at the hyperparameters\n",
    "# -\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Questions\n",
    "\n",
    "# Quelle architecture ?\n",
    "#\n",
    "# Comment éviter l'overfitting ?\n",
    "#\n",
    "# Comment exploiter les avantages de l'IA ?\n",
    "#\n",
    "# Choix de l'optimizer + regularizer ? + Implémentation ?\n",
    "#\n",
    "# Implémentation de système d'EDP à plusieurs inconnues (étant des fonctions bien sûr) ? (Est-ce que c'est utile ça ? Par curiosité)\n",
    "#\n",
    "# Plus rapide ? Comment enlever les boucles `for` ? => mini_batch_gradient_descent ? done\n",
    "#\n",
    "# Besoin de batch_normalization ? + autres hyperparamètres ?\n",
    "\n",
    "# # Idées\n",
    "\n",
    "# Ajout de bruit en entrée contre l'overfitting\n",
    "#\n",
    "# Une sortie par inconnue\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few train parameters to adjust\n",
    "# use learning_rate = 1e-2, batch_size = 1000 for dummy F\n",
    "###############\n",
    "def generate_random_config(trial_id,grid_length):\n",
    "    n_hidden_layers = np.random.randint(2,7)\n",
    "    l_units = [5*np.random.randint(1,7) for _ in range(n_hidden_layers)]\n",
    "    noise = np.random.randint(2)\n",
    "\n",
    "    config_model = {\n",
    "        'l_units': l_units,\n",
    "        'noise': noise,\n",
    "        'learning_rate': 1e-2,\n",
    "        'optimizer': \"Adam\"\n",
    "    }\n",
    "\n",
    "\n",
    "    n_trains = 50*np.random.randint(2,5)\n",
    "    config_training = {\n",
    "        \"epochs_max\": 5000,\n",
    "        \"n_trains\": n_trains,\n",
    "        \"batch_size\": 8192,\n",
    "        \"display_step\": 10,\n",
    "        \"tol\": 1e-6,\n",
    "        \"patience\": 50\n",
    "    }\n",
    "\n",
    "    config = {\n",
    "        \"trial_id\": trial_id,\n",
    "        \"grid_length\":grid_length, # do not change anything, here to inform\n",
    "        \"config_training\": config_training,\n",
    "        \"config_model\": config_model\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "max_trials = 100\n",
    "id_add=100\n",
    "def randomTuning(max_trials,id_add):\n",
    "    for trial_id in range(max_trials):\n",
    "        config = generate_random_config(trial_id,grid_length)\n",
    "        try_config(config,id_add=id_add)\n",
    "\n",
    "randomTuning(max_trials,id_add) ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " {'trial_id': 0, 'grid_length': 1000, 'remark': 'do overfit', 'config_training': {'epochs_max': 5000, 'n_trains': 10000, 'batch_size': 65536, 'display_step': 100, 'tol': 1e-06, 'patience': 5000}, 'config_model': {'l_units': [30, 30], 'noise': 0, 'learning_rate': 0.0001, 'optimizer': 'Adam'}}\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 30)                90        \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1)                 30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,050\n",
      "Trainable params: 1,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch: 1 ....................................................................................................mean_loss: 0.01926151 val_mae: 0.40817046 WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "duration epoch: 231.39247179031372\n",
      "\n",
      "epoch: 2 ....................................................................................................mean_loss: 0.020836184 val_mae: 0.40400857 WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "duration epoch: 231.87281036376953\n",
      "\n",
      "epoch: 3 ....................................................................................................mean_loss: 0.013686223 val_mae: 0.40738142 WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "duration epoch: 244.179349899292\n",
      "\n",
      "epoch: 4 ....................................................................................................mean_loss: 0.013381184 val_mae: 0.4116396 WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "duration epoch: 247.49915432929993\n",
      "\n",
      "epoch: 5 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\differentiate\\NN_method_3.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=8'>9</a>\u001b[0m config_training \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepochs_max\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m5000\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=10'>11</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_trains\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m10000\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=14'>15</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpatience\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m5000\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=15'>16</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=17'>18</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrial_id\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=19'>20</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgrid_length\u001b[39m\u001b[39m\"\u001b[39m:grid_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=22'>23</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconfig_model\u001b[39m\u001b[39m\"\u001b[39m: config_model\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=23'>24</a>\u001b[0m     }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000003?line=25'>26</a>\u001b[0m try_config(config,id_add\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\differentiate\\NN_method_3.ipynb Cell 2'\u001b[0m in \u001b[0;36mtry_config\u001b[1;34m(config, id_add)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=112'>113</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=114'>115</a>\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(tf_coords\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=115'>116</a>\u001b[0m tf_sample_coords \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor([tf_coords[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=116'>117</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_trains):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=117'>118</a>\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39m%\u001b[39m display_step \u001b[39m==\u001b[39m display_step\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\differentiate\\NN_method_3.ipynb Cell 2'\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=112'>113</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=114'>115</a>\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(tf_coords\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=115'>116</a>\u001b[0m tf_sample_coords \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor([tf_coords[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=116'>117</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_trains):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jtros/CS/cours/PoleProjet/FormationRecherche/Tsunami/TP/sceance4/Tsunami/differentiate/NN_method_3.ipynb#ch0000001?line=117'>118</a>\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39m%\u001b[39m display_step \u001b[39m==\u001b[39m display_step\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1097\u001b[0m, in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m   1095\u001b[0m   var_empty \u001b[39m=\u001b[39m constant([], dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\n\u001b[0;32m   1096\u001b[0m   packed_begin \u001b[39m=\u001b[39m packed_end \u001b[39m=\u001b[39m packed_strides \u001b[39m=\u001b[39m var_empty\n\u001b[1;32m-> 1097\u001b[0m \u001b[39mreturn\u001b[39;00m strided_slice(\n\u001b[0;32m   1098\u001b[0m     tensor,\n\u001b[0;32m   1099\u001b[0m     packed_begin,\n\u001b[0;32m   1100\u001b[0m     packed_end,\n\u001b[0;32m   1101\u001b[0m     packed_strides,\n\u001b[0;32m   1102\u001b[0m     begin_mask\u001b[39m=\u001b[39;49mbegin_mask,\n\u001b[0;32m   1103\u001b[0m     end_mask\u001b[39m=\u001b[39;49mend_mask,\n\u001b[0;32m   1104\u001b[0m     shrink_axis_mask\u001b[39m=\u001b[39;49mshrink_axis_mask,\n\u001b[0;32m   1105\u001b[0m     new_axis_mask\u001b[39m=\u001b[39;49mnew_axis_mask,\n\u001b[0;32m   1106\u001b[0m     ellipsis_mask\u001b[39m=\u001b[39;49mellipsis_mask,\n\u001b[0;32m   1107\u001b[0m     var\u001b[39m=\u001b[39;49mvar,\n\u001b[0;32m   1108\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1270\u001b[0m, in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[39mif\u001b[39;00m strides \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1268\u001b[0m   strides \u001b[39m=\u001b[39m ones_like(begin)\n\u001b[1;32m-> 1270\u001b[0m op \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mstrided_slice(\n\u001b[0;32m   1271\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49minput_,\n\u001b[0;32m   1272\u001b[0m     begin\u001b[39m=\u001b[39;49mbegin,\n\u001b[0;32m   1273\u001b[0m     end\u001b[39m=\u001b[39;49mend,\n\u001b[0;32m   1274\u001b[0m     strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[0;32m   1275\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1276\u001b[0m     begin_mask\u001b[39m=\u001b[39;49mbegin_mask,\n\u001b[0;32m   1277\u001b[0m     end_mask\u001b[39m=\u001b[39;49mend_mask,\n\u001b[0;32m   1278\u001b[0m     ellipsis_mask\u001b[39m=\u001b[39;49mellipsis_mask,\n\u001b[0;32m   1279\u001b[0m     new_axis_mask\u001b[39m=\u001b[39;49mnew_axis_mask,\n\u001b[0;32m   1280\u001b[0m     shrink_axis_mask\u001b[39m=\u001b[39;49mshrink_axis_mask)\n\u001b[0;32m   1282\u001b[0m parent_name \u001b[39m=\u001b[39m name\n\u001b[0;32m   1284\u001b[0m \u001b[39mif\u001b[39;00m var \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\PoleProjet\\FormationRecherche\\Tsunami\\TP\\sceance4\\Tsunami\\venv_tsunami\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10664\u001b[0m, in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10662\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m  10663\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m> 10664\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m  10665\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mStridedSlice\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, begin, end, strides, \u001b[39m\"\u001b[39;49m\u001b[39mbegin_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m  10666\u001b[0m       begin_mask, \u001b[39m\"\u001b[39;49m\u001b[39mend_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, end_mask, \u001b[39m\"\u001b[39;49m\u001b[39mellipsis_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, ellipsis_mask,\n\u001b[0;32m  10667\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mnew_axis_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, new_axis_mask, \u001b[39m\"\u001b[39;49m\u001b[39mshrink_axis_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, shrink_axis_mask)\n\u001b[0;32m  10668\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m  10669\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_model = {\n",
    "        'l_units': [30,30],\n",
    "        'noise': 0,\n",
    "        'learning_rate': 1e-4,\n",
    "        'optimizer': \"Adam\"\n",
    "    }\n",
    "\n",
    "\n",
    "config_training = {\n",
    "        \"epochs_max\": 5000,\n",
    "        \"n_trains\": 10000,\n",
    "        \"batch_size\": 65536,\n",
    "        \"display_step\": 100,\n",
    "        \"tol\": 1e-6,\n",
    "        \"patience\": 5000\n",
    "    }\n",
    "\n",
    "config = {\n",
    "        \"trial_id\": 0,\n",
    "        \"grid_length\":grid_length,\n",
    "        'remark':'do overfit',\n",
    "        \"config_training\": config_training,\n",
    "        \"config_model\": config_model\n",
    "    }\n",
    "\n",
    "try_config(config,id_add=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv_tsunami': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed45ddf3773bc83c4383a1a855f930bc53f0b7f7e989df0f3182f9bb62c1b41f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
