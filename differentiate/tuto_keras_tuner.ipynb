{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner Tutorial\n",
    "\n",
    "ref: https://www.tensorflow.org/tutorials/keras/keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`! pip install -q -U keras-tuner` to install keras tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "In this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel.\n",
    "\n",
    "You can define a hypermodel through two approaches:\n",
    "\n",
    "    By using a model builder function\n",
    "    By subclassing the HyperModel class of the Keras Tuner API\n",
    "\n",
    "You can also use two pre-defined HyperModel classes - HyperXception and HyperResNet for computer vision applications.\n",
    "\n",
    "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a <ins>**compiled model**</ins> and uses hyperparameters you define inline to hypertune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-512\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy']) \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the tuner and perform hypertuning\n",
    "\n",
    "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - RandomSearch, Hyperband, BayesianOptimization, and Sklearn. In this tutorial, you use the Hyperband tuner.\n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### details:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the search space, we need to select a tuner class to run the search.\n",
    "\n",
    "To initialize the tuner, we need to specify several arguments in the initializer.\n",
    "\n",
    "\n",
    "`hypermodel`. The model-building function, which is build_model in our case.\n",
    "\n",
    "`objective`. The name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics). We will introduce how to use custom metrics later in this tutorial.\n",
    "`max_trials`. The total number of trials to run during the search.\n",
    "\n",
    "`executions_per_trial`. The number of models that should be built and fit for each trial. Different trials have different hyperparameter values. The executions within the same trial have the same hyperparameter values. The purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If you want to get results faster, you could set executions_per_trial=1 (single round of training for each model configuration).\n",
    "\n",
    "`overwrite`. Control whether to overwrite the previous results in the same directory or resume the previous search instead. Here we set overwrite=True to start a new search and ignore any previous results.\n",
    "\n",
    "`directory`. A path to a directory for storing the search results.\n",
    "\n",
    "`project_name`. The name of the sub-directory in the directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='./differentiate/results_hypertuning',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + logfactor(max_epochs) and rounding it up to the nearest integer.\n",
    "\n",
    "Create a callback to stop training early after reaching a certain value for the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.\n",
    "\n",
    "<ins>The issue</ins> : All the arguments passed to search is passed to model.fit() in each execution. Remember to pass validation_data to evaluate the model. Hence, the need of a compiled model... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "# tuner.search(img_train, label_train, epochs=50,validation_data=(x_val, y_val), callbacks=[stop_early])\n",
    "# with predefined x_val, y_val also works \n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of this search :\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-instantiate the hypermodel and train it with the optimal number of epochs from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this tutorial, evaluate the hypermodel on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = hypermodel.evaluate(img_test, label_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper into it\n",
    "\n",
    "ref: https://keras.io/guides/keras_tuner/getting_started/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other types of hyperparameters as well. We can define multiple hyperparameters in the function. In the following code, we tune the whether to use a Dropout layer with `hp.Boolean()`, tune which activation function to use with `hp.Choice()`, tune the learning rate of the optimizer with `hp.Float()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            # Tune number of units.\n",
    "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "            # Tune the activation function to use.\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        )\n",
    "    )\n",
    "    # Tune whether to use dropout.\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    # Define the optimizer learning rate as a hyperparameter.\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### how to use an existing code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_existing_code(units, activation, dropout, lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=units, activation=activation))\n",
    "    if dropout:\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int(\"units\", min_value=32, max_value=512, step=32)\n",
    "    activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    # call existing model-building code with the hyperparameter values.\n",
    "    model = call_existing_code(\n",
    "        units=units, activation=activation, dropout=dropout, lr=lr\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### an example of creating conditional hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the tuning results using TensorBoard and HParams plugin. For more information, please following this link: https://keras.io/guides/keras_tuner/visualize_tuning/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune model training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the model building process, we need to subclass the HyperModel class, which also makes it easy to share and reuse hypermodels.\n",
    "\n",
    "We need to override HyperModel.build() and HyperModel.fit() to tune the model building and training process respectively. A HyperModel.build() method is the same as the model-building function, which creates a Keras model using the hyperparameters and returns it.\n",
    "\n",
    "In HyperModel.fit(), you can access the model returned by HyperModel.build(),hp and all the arguments passed to search(). You need to train the model and return the training history.\n",
    "\n",
    "In the following code, we will tune the shuffle argument in model.fit().\n",
    "\n",
    "It is generally not needed to tune the number of epochs because a built-in callback is passed to model.fit() to save the model at its best epoch evaluated by the validation_data.\n",
    "\n",
    "    Note: The **kwargs should always be passed to model.fit() because it contains the callbacks for model saving and tensorboard plugins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            # Tune whether to shuffle the data in each epoch.\n",
    "            shuffle=hp.Boolean(\"shuffle\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "hp = keras_tuner.HyperParameters()\n",
    "hypermodel = MyHyperModel()\n",
    "model = hypermodel.build(hp)\n",
    "hypermodel.fit(hp, model, np.random.rand(100, 28, 28), np.random.rand(100, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune end-to-end workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, it is hard to align your code into build and fit functions. You can also keep your end-to-end workflow in one place by overriding Tuner.run_trial(), which gives you full control of a trial. You can see it as a black-box optimizer for anything.\n",
    "\n",
    "Tune any function\n",
    "\n",
    "For example, you can find a value of x, which minimizes f(x)=x*x+1. In the following code, we just define x as a hyperparameter, and return f(x) as the objective value. The hypermodel and objective argument for initializing the tuner can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 00s]\n",
      "default_objective: 1.935912639314918\n",
      "\n",
      "Best default_objective So Far: 1.0079193308637155\n",
      "Total elapsed time: 00h 00m 00s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "-0.08899062233581478\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner\n",
    "\n",
    "class MyTuner(keras_tuner.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # Get the hp from trial.\n",
    "        hp = trial.hyperparameters\n",
    "        # Define \"x\" as a hyperparameter.\n",
    "        x = hp.Float(\"x\", min_value=-1.0, max_value=1.0)\n",
    "        # Return the objective value to minimize.\n",
    "        return x * x + 1\n",
    "\n",
    "\n",
    "tuner = MyTuner(\n",
    "    # No hypermodel or objective specified.\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"tune_anything\",\n",
    ")\n",
    "\n",
    "# No need to pass anything to search()\n",
    "# unless you use them in run_trial().\n",
    "tuner.search()\n",
    "print(tuner.get_best_hyperparameters()[0].get(\"x\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BibTex entry for KerasTuner\n",
    "\n",
    "@misc{omalley2019kerastuner,\n",
    "    title        = {KerasTuner},\n",
    "    author       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others},\n",
    "    year         = 2019,\n",
    "    howpublished = {\\url{https://github.com/keras-team/keras-tuner}}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed45ddf3773bc83c4383a1a855f930bc53f0b7f7e989df0f3182f9bb62c1b41f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv_tsunami': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
